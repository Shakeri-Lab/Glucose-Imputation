# ğŸ§  Algorithmic Methodology: Realistic Missingness as a Generative Process

## ğŸ¯ Motivation

Missing data in real-world time-series is **not random**. It follows repeatable patterns driven by physiology, behavior, and hardware constraints. Treating missingness as MCAR hides this structure and produces misleading evaluations.

This methodology models missingness as a **learnable stochastic process** and then **re-samples it algorithmically**.

> ğŸ§© Goal: generate missingness that behaves like reality, not like noise.

---

## ğŸ§± Core Decomposition

Missingness is represented as a sequence of **gap events**. Each gap is defined by three independent components:

1. â° **When** the gap starts
2. ğŸ§¬ **What type** of gap it is (atomic vs sustained)
3. ğŸ“ **How long** it lasts

Formally:

P(missingness) = P(start time) Â· P(gap type) Â· P(duration | type)

This separation keeps the model interpretable and modular.

---

## ğŸ§  Modeling Assumptions

To remain simple and stable, the model assumes:

* âŒš Fixed temporal grid (Î”t = 5 minutes)
* ğŸ”— Missingness appears as contiguous gaps
* ğŸ§© Timing and duration are conditionally independent
* ğŸ‘¥ Population-level statistics are shared

These assumptions are explicit and can be relaxed later.

---

## ğŸ”¬ Stage I â€” Learning Missingness from Data

### ğŸ§­ Step 1: Temporal Alignment

All signals are projected onto a complete, fixed-resolution timeline. After this step, missingness is a **binary process** over time rather than an artifact of irregular sampling.

---

### ğŸ§¹ Step 2: Structural Day Validation

Days with extremely low data coverage are removed. These days usually represent non-wear or logging failure rather than genuine sensor gaps.

âœ” Only days exceeding a minimum coverage threshold are retained.

---

### ğŸ§¾ Step 3: Gap Event Representation

Missingness is summarized as a set of gap events:

G = {(tâ‚, â„“â‚), (tâ‚‚, â„“â‚‚), â€¦}

where:

* táµ¢ = gap start time
* â„“áµ¢ = gap duration

This converts raw binary sequences into a **marked point process**.

---

### â° Step 4: Learning *When* Gaps Start

We estimate the probability that a gap begins at each hour of the day:

P(start | hour = h)

This captures circadian structure such as nighttime compression losses or daytime activity artifacts.

ğŸ“Š No parametric assumptionsâ€”only empirical counting.

---

### ğŸ“ Step 5: Learning *How Long* Gaps Last

Empirically, gap durations fall into two regimes.

#### âš¡ Atomic Gaps

Single-interval gaps (â„“ = Î”t) are modeled explicitly using a Bernoulli probability.

These represent brief transmission glitches.

#### ğŸ§© Sustained Gaps

Longer gaps are modeled using a mixture distribution:

f(â„“) = wâ‚Â·Exp(â„“) + wâ‚‚Â·Gauss(â„“) + wâ‚ƒÂ·Uniform(â„“)

Interpretation:

* Exp â†’ short outages
* Gauss â†’ structured physiological gaps
* Uniform â†’ rare long-tail events

---

## ğŸ§ª Stage II â€” Generating New Missingness

### ğŸ¯ Step 6: Gap Triggering

For each hour in the target signal:

* Draw a Bernoulli trial using P(start | hour)
* If successful, initiate a gap

---

### ğŸ² Step 7: Duration Sampling

If a gap is triggered:

* With probability P(â„“ = Î”t), generate an atomic gap
* Otherwise, sample â„“ from the mixture distribution

Durations are clipped to plausible bounds.

---

### ğŸ§© Step 8: Mask Realization

Each sampled gap is instantiated as a contiguous missing segment on the temporal grid. The output is a binary missingness mask.

---

## ğŸ§  Algorithm Summary (Pseudocode)

```
Algorithm LearnAndGenerateMissingness
Input: Time-series dataset D
Output: Missingness mask M

1. Align all signals to fixed grid
2. Remove low-coverage days
3. Extract gap events G = {(táµ¢, â„“áµ¢)}
4. Estimate P(start | hour)
5. Estimate P(â„“ = Î”t)
6. Fit mixture model for â„“ > Î”t

7. For each hour h in new signal:
      if Bernoulli(P(start | h)):
          if Bernoulli(P(â„“ = Î”t)):
              â„“ â† Î”t
          else:
              â„“ â† Sample from mixture
          Apply gap of length â„“

Return M
```

---

## ğŸ§­ Conceptual Diagram

```
Raw Signal
   â”‚
   â–¼
[ Temporal Alignment ]
   â”‚
   â–¼
[ Gap Extraction ]
   â”‚
   â–¼
[ Learn Start-Time PMF ]
[ Learn Duration Model ]
   â”‚
   â–¼
[ Gap Trigger + Duration Sampler ]
   â”‚
   â–¼
Generated Missingness Mask
```

---

## ğŸŒ± Why This Design Works

* ğŸ§© Clear separation of concerns
* ğŸ“Š Fully data-driven
* ğŸ” Interpretable at every stage
* ğŸ§ª Produces deployment-relevant missingness

---

## ğŸ§  Guiding Principle

Missingness is **behavior**, not noise. Modeling it explicitly leads to fairer benchmarks and more robust algorithms.
