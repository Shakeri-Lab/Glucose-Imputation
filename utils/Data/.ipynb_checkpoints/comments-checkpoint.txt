# import numpy as np
# from pygrinder import mcar, seq_missing, block_missing

# def create_missing(cgm, rate, pattern='mcar', **kwargs):
#     """ Create missingness based on patterns. """
#     assert pattern in ['mcar', 'seq_missing', 'block_missing'], 'pattern should be mcar, seq_missing or block_missing.'
#     if pattern == 'mcar':
#         return mcar(cgm, rate)
#     elif pattern == 'seq_missing':
#         return seq_missing(np.expand_dims(cgm, axis=0), rate, seq_len=kwargs['seq_len']).squeeze(0)
#     else:
#         return block_missing(np.expand_dims(cgm, axis=0), factor=rate, block_len=kwargs['block_len'], block_width=kwargs['block_width']).squeeze(0)
        





# # import torch, math
# # import numpy as np
# # import pandas as pd
# # from .missing import create_missing

# # class CGMDataset:
# #     """ CGM Dataset preparation. """
# #     def __init__(self, seq_len, data_dir, miss_cfg, stride=12, data_cols=['cgm'], unique_col='pid', missing_enabled=False):
# #         self.seq_len = seq_len
# #         self.stride = stride
# #         self.data_dir = data_dir
# #         self.data_cols = data_cols
# #         self.unique_col = unique_col
# #         self.miss_cfg = miss_cfg
# #         self.missing_enabled = missing_enabled
       
# #     def load_data(self):
# #         df = pd.read_csv(self.data_dir)
# #         return df if set(self.data_cols + [self.unique_col]).issubset(df.columns) else None
   
# #     def get_numpy_arrays(self, df):        
# #         data, pid = [df[col].values for col in self.data_cols], df[self.unique_col].values
# #         return data, pid

# #     def valid_pid(self, pid, start, length):
# #         return (pid[start:start + length] == pid[start]).all()

# #     def has_nan(self, arrays, start, length):
# #         return any(np.isnan(arr[start:start + length]).any() for arr in arrays)

# #     def get_input_slice(self, arrays, start):
# #         return np.stack([arr[start:start + self.seq_len] for arr in arrays], axis=1)

# #     def skew_norm(self, x, skew, min_val, max_val):
# #         """Normalize the input x to the range [0, 1] using a skewed function."""
# #         x = np.array(x) # Ensure input is numpy array
# #         if x.ndim == 1:
# #             x = np.expand_dims(x, 0)
# #         x = np.maximum(np.minimum(x, max_val), min_val)
# #         return np.power((x - min_val) / (max_val - min_val), skew)
    
# #     def build_dataset(self):
# #         df = self.load_data()
# #         if df is None: return None, 0
# #         data, pid = self.get_numpy_arrays(df)
# #         x_list = []
        
# #         for day_start in range(0, len(df) - 288 + 1, 288):
# #             for i in range(day_start, day_start + 288 - self.seq_len + 1, self.stride):
# #                 if not self.valid_pid(pid, i, self.seq_len): continue
# #                 if self.has_nan(data, i, self.seq_len): continue
    
# #                 x = self.get_input_slice(data, i)
# #                 x = self.skew_norm(x, 1, 40, 400)
# #                 if self.missing_enabled:
# #                     x = create_missing(x, self.miss_cfg["rate"], self.miss_cfg["pattern"], **{k: v for k, v in self.miss_cfg.items() if k not in ("rate", "pattern")})
# #                 x_list.append(x)
    
# #         x_samples = np.stack(x_list, axis=0).astype(float)
# #         return x_samples, x_samples.shape[0]







# import torch, math
# import numpy as np
# import pandas as pd
# from torch.utils.data import Dataset
# from .missing import simulate_missingness_pipeline
# from .test_missing import simulate_missingness_test_pipeline

# class CGMDataset(Dataset):
#     def __init__(self, data_path, seq_len=288, stride=12, 
#                  missing_enabled=True, miss_cfg=None, is_evaluate=False):
#         self.seq_len = seq_len
#         self.stride = stride
#         self.missing_enabled = missing_enabled
#         self.miss_cfg = miss_cfg or {}
        
#         self.df = pd.read_csv(data_path)
        
#         if self.missing_enabled:
#             if not is_evaluate:
#                 self.df = simulate_missingness_pipeline(
#                     self.df,
#                     min_pct=self.miss_cfg.get('min_pct', 0.1),
#                     max_pct=self.miss_cfg.get('max_pct', 0.3),
#                     probs=self.miss_cfg.get('probs', {'mnar': 0.5, 'pisa': 0.3, 'mcar': 0.8}),
#                 )
#             else:
#                 self.df = simulate_missingness_test_pipeline(
#                     self.df,
#                     max_pct=self.miss_cfg.get('max_pct', 0.3),
#                     probs=self.miss_cfg.get('probs', {'mnar': 0.5, 'pisa': 0.3, 'mcar': 0.8}),
#                 )                
#         else:
#             self.df['cgm_simulated'] = self.df['cgm'].copy() 

#         self.samples = self._build_samples()

#     def _absolute_time_encoding(self, indices, T_day=288):
#         t = indices.float() / float(T_day)
#         return torch.stack([torch.sin(2 * math.pi * t), torch.cos(2 * math.pi * t)], dim=-1)

#     def _skew_norm(self, x, skew=1, min_val=40, max_val=400):
#         x = np.array(x)
#         x = np.maximum(np.minimum(x, max_val), min_val)
#         return np.power((x - min_val) / (max_val - min_val), skew)

#     def _build_samples(self):
#         samples = []
#         for pid, group in self.df.groupby('pid'):
#             in_vals, gt_vals = group['cgm_simulated'].values, group['cgm'].values
            
#             dates = pd.to_datetime(group['date'])
#             time_indices = (dates.dt.hour * 60 + dates.dt.minute) // 5
#             time_indices = torch.tensor(time_indices.values)
#             time_embeds = self._absolute_time_encoding(time_indices).numpy()

#             num_points = len(group)
#             for i in range(0, num_points - self.seq_len + 1, self.stride):
#                 slice_gt = gt_vals[i : i + self.seq_len]
#                 if np.isnan(slice_gt).any(): continue

#                 slice_in = in_vals[i : i + self.seq_len]
#                 slice_time = time_embeds[i : i + self.seq_len]
                
#                 # Normalize
#                 norm_in = self._skew_norm(slice_in) 
#                 norm_gt = self._skew_norm(slice_gt)
                
#                 sample = np.column_stack([norm_in, slice_time, norm_gt]) # [Input, Sin, Cos, Truth]
#                 samples.append(sample)
                
#         return np.array(samples)

#     def __len__(self):
#         return len(self.samples)
#     def __getitem__(self, idx):
#         return torch.FloatTensor(self.samples[idx])




# import torch, math
# import numpy as np
# import pandas as pd
# from torch.utils.data import Dataset
# from .missing import simulate_missingness_pipeline
# from .test_missing import simulate_missingness_test_pipeline

# class CGMDataset(Dataset):
#     def __init__(self, data_path, seq_len=288, stride=12, 
#                  missing_enabled=True, miss_cfg=None, is_evaluate=False, is_dclp3=False):
#         self.seq_len = seq_len
#         self.stride = stride
#         self.missing_enabled = missing_enabled
#         self.miss_cfg = miss_cfg or {}
        
#         self.df = pd.read_csv(data_path)
        
#         if self.missing_enabled:
#             if not is_evaluate:
#                 self.df = simulate_missingness_pipeline(
#                     self.df,
#                     min_pct=self.miss_cfg.get('min_pct', 0.1),
#                     max_pct=self.miss_cfg.get('max_pct', 0.3),
#                     probs=self.miss_cfg.get('probs', {'mnar': 0.5, 'pisa': 0.3, 'mcar': 0.8}),
#                 )
#             else:
#                 self.df = simulate_missingness_test_pipeline(
#                     self.df,
#                     max_pct=self.miss_cfg.get('max_pct', 0.3),
#                     probs=self.miss_cfg.get('probs', {'mnar': 0.5, 'pisa': 0.3, 'mcar': 0.8}),
#                 )                
#         else:
#             self.df['cgm_simulated'] = self.df['cgm'].copy() 

#         self.samples = self._build_samples_dclp3() if is_dclp3 else self._build_samples()

#     def _absolute_time_encoding(self, indices, T_day=288):
#         t = indices.float() / float(T_day)
#         return torch.stack([torch.sin(2 * math.pi * t), torch.cos(2 * math.pi * t)], dim=-1)

#     def _skew_norm(self, x, skew=1, min_val=40, max_val=400):
#         x = np.array(x)
#         x = np.maximum(np.minimum(x, max_val), min_val)
#         return np.power((x - min_val) / (max_val - min_val), skew)

#     def _build_samples_dclp3(self):
#         samples = []
#         for pid, group in self.df.groupby('pat_id'):
#             for seq_id, episode in group.groupby('seq_id'):
#                 in_vals, gt_vals = episode['cgm_simulated'].values, episode['cgm'].values
                
#                 dates = pd.to_datetime(episode['date'])
#                 time_indices = (dates.dt.hour * 60 + dates.dt.minute) // 5
#                 time_indices = torch.tensor(time_indices.values)
#                 time_embeds = self._absolute_time_encoding(time_indices).numpy()
    
#                 num_points = len(episode)
#                 for i in range(0, num_points - self.seq_len + 1, self.stride):
#                     slice_gt = gt_vals[i : i + self.seq_len]
#                     if np.isnan(slice_gt).any(): continue
    
#                     slice_in = in_vals[i : i + self.seq_len]
#                     slice_time = time_embeds[i : i + self.seq_len]
                    
#                     # Normalize
#                     norm_in = self._skew_norm(slice_in) 
#                     norm_gt = self._skew_norm(slice_gt)
                    
#                     sample = np.column_stack([norm_in, slice_time, norm_gt]) # [Input, Sin, Cos, Truth]
#                     samples.append(sample)
                
#         return np.array(samples)

#     def _build_samples(self):
#         samples = []
#         for pid, group in self.df.groupby('pid'):
#             in_vals, gt_vals = group['cgm_simulated'].values, group['cgm'].values
            
#             dates = pd.to_datetime(group['date'])
#             time_indices = (dates.dt.hour * 60 + dates.dt.minute) // 5
#             time_indices = torch.tensor(time_indices.values)
#             time_embeds = self._absolute_time_encoding(time_indices).numpy()

#             num_points = len(group)
#             for i in range(0, num_points - self.seq_len + 1, self.stride):
#                 slice_gt = gt_vals[i : i + self.seq_len]
#                 if np.isnan(slice_gt).any(): continue

#                 slice_in = in_vals[i : i + self.seq_len]
#                 slice_time = time_embeds[i : i + self.seq_len]
                
#                 # Normalize
#                 norm_in = self._skew_norm(slice_in) 
#                 norm_gt = self._skew_norm(slice_gt)
                
#                 sample = np.column_stack([norm_in, slice_time, norm_gt]) # [Input, Sin, Cos, Truth]
#                 samples.append(sample)
                
#         return np.array(samples)

    
#     def __len__(self):
#         return len(self.samples)
#     def __getitem__(self, idx):
#         return torch.FloatTensor(self.samples[idx])






# import torch
# import math
# import numpy as np
# import pandas as pd


# def apply_mnar_values(drop_mask, day_df, max_points):
#     """
#     Simulates NMAR (Not Missing At Random) by masking values based on their 
#     actual glucose level (High > 150 or Low < 70).
#     """
#     #    Thresholds: < 70 mg/dL or > 150 mg/dL 
#     triggers = day_df[(day_df['cgm'] > 150) | (day_df['cgm'] < 70)].index.tolist()
#     np.random.shuffle(triggers)
    
#     for trigger_idx in triggers:
#         if drop_mask.sum() >= max_points:
#             break
            
#         loc_idx = day_df.index.get_loc(trigger_idx)
#         duration = int(np.random.randint(30, 90) / 5)
        
#         # Calculate remaining quota to avoid over-masking
#         remaining = max_points - drop_mask.sum()
#         if duration > remaining:
#             duration = remaining
            
#         if duration > 0:
#             end_loc = min(loc_idx + duration, len(day_df))
#             drop_mask[loc_idx : end_loc] = True
            
#     return drop_mask


# def apply_mar_pisa(drop_mask, day_df, max_points):
#     """ PISA simulation. """
#     remaining = max_points - drop_mask.sum()
#     if remaining <= 0: return drop_mask

#     # Define sleep mask
#     is_sleep = (day_df['date'].dt.hour <= 7) | (day_df['date'].dt.hour >= 23)
#     sleep_pos = np.where(is_sleep)[0]
    
#     if len(sleep_pos) > 0:
#         pisa_start = np.random.choice(sleep_pos)
#         duration = int(np.random.randint(20, 60) / 5)
        
#         if duration > remaining: duration = remaining
        
#         if duration > 0:
#             end_loc = min(pisa_start + duration, len(day_df))
#             intended_window = is_sleep.iloc[pisa_start : end_loc]
            
#             if (~intended_window).any():
#                 first_wake_idx = np.where(~intended_window)[0][0]
#                 end_loc = pisa_start + first_wake_idx
            
#             # Apply mask
#             drop_mask[pisa_start : end_loc] = True
            
#     return drop_mask


# def apply_mcar_random_noise(drop_mask, min_points, max_points):
#     """ Randomly Noises. """
#     current_drops = drop_mask.sum()
#     needed = 0
    
#     if current_drops < min_points:
#         needed = min_points - current_drops
#     elif current_drops < max_points:
#         remaining = max_points - current_drops
#         needed = int(np.random.rand() * remaining * 0.2)
        
#     if needed > 0:
#         safe_pos = np.where(drop_mask == False)[0]
#         needed = min(needed, len(safe_pos))
#         if needed > 0:
#             mcar_pos = np.random.choice(safe_pos, size=needed, replace=False)
#             drop_mask[mcar_pos] = True
#     return drop_mask

# def process_single_day(day_df, min_pct, max_pct, probs):
#     df_day = day_df.copy()
#     total_points = len(df_day)
#     drop_mask = np.zeros(total_points, dtype=bool)
    
#     max_points = int(total_points * max_pct)
#     min_points = int(total_points * min_pct)
    
#     active_mnar     = np.random.rand() < probs.get('mnar', 0.5)
#     active_pisa     = np.random.rand() < probs.get('pisa', 0.3)
#     active_mcar     = np.random.rand() < probs.get('mcar', 0.8)
    
#     if not (active_mnar or active_pisa or active_mcar):
#         return df_day
#     # Apply Logic
#     if active_mnar:
#         drop_mask = apply_mnar_values(drop_mask, df_day, max_points)
#     if active_pisa:
#         drop_mask = apply_mar_pisa(drop_mask, df_day, max_points)
#     if active_mcar:
#         drop_mask = apply_mcar_random_noise(drop_mask, min_points, max_points)

#     col_loc = df_day.columns.get_loc('cgm_simulated')
#     df_day.iloc[drop_mask, col_loc] = np.nan
#     return df_day

# def simulate_missingness_pipeline(df, min_pct, max_pct, probs):
#     df = df.copy()
#     if not pd.api.types.is_datetime64_any_dtype(df['date']):
#         df['date'] = pd.to_datetime(df['date'])
    
#     df['cgm_simulated'] = df['cgm'].copy()
    
#     def process_patient(patient_df):
#         day_groups = patient_df.groupby(patient_df['date'].dt.date)
#         results = []
#         for date, day_data in day_groups:
#             processed_day = process_single_day(day_data, min_pct, max_pct, probs)
#             results.append(processed_day)
#         return pd.concat(results)

#     if 'pid' in df.columns:
#         final_df = df.groupby('pid').apply(process_patient).reset_index(drop=True)
#     else:
#         final_df = process_patient(df)
#     return final_df





# import torch, math
# import numpy as np
# import pandas as pd
# from torch.utils.data import Dataset
# from .missing import simulate_missingness_pipeline

# class CGMDataset(Dataset):
#     def __init__(self, data_path, seq_len=288, stride=12, 
#                  missing_enabled=True, miss_cfg=None, is_dclp3=False):
#         self.seq_len = seq_len
#         self.stride = stride
#         self.missing_enabled = missing_enabled
#         self.miss_cfg = miss_cfg or {}
        
#         self.df = pd.read_csv(data_path)
        
#         if self.missing_enabled:
#             self.df = simulate_missingness_pipeline(
#                 self.df,
#                 max_pct=self.miss_cfg.get('max_pct', 0.3),
#                 probs=self.miss_cfg.get('probs', {'mnar': 0.5, 'meal': 0.3, 'mcar': 0.8}),
#             )                
#         else:
#             self.df['cgm_simulated'] = self.df['cgm'].copy() 

#         self.samples = self._build_samples_dclp3() if is_dclp3 else self._build_samples()

#     def _absolute_time_encoding(self, indices, T_day=288):
#         t = indices.float() / float(T_day)
#         return torch.stack([torch.sin(2 * math.pi * t), torch.cos(2 * math.pi * t)], dim=-1)

#     def _skew_norm(self, x, skew=1, min_val=40, max_val=400):
#         x = np.array(x)
#         x = np.maximum(np.minimum(x, max_val), min_val)
#         return np.power((x - min_val) / (max_val - min_val), skew)

#     def _build_samples_dclp3(self):
#         samples = []
#         for pid, group in self.df.groupby('pat_id'):
#             for seq_id, episode in group.groupby('seq_id'):
#                 in_vals, gt_vals = episode['cgm_simulated'].values, episode['cgm'].values
#                 meal_vals = np.where(episode['meal'].values > 0, 1, 0)
                
#                 dates = pd.to_datetime(episode['date'])
#                 time_indices = (dates.dt.hour * 60 + dates.dt.minute) // 5
#                 time_indices = torch.tensor(time_indices.values)
#                 time_embeds = self._absolute_time_encoding(time_indices).numpy()
    
#                 num_points = len(episode)
#                 for i in range(0, num_points - self.seq_len + 1, self.stride):
#                     slice_gt = gt_vals[i : i + self.seq_len]
#                     if np.isnan(slice_gt).any(): continue
    
#                     slice_in = in_vals[i : i + self.seq_len]
#                     slice_meal = meal_vals[i : i + self.seq_len]
#                     slice_time = time_embeds[i : i + self.seq_len]
                    
#                     # Normalize
#                     norm_in = self._skew_norm(slice_in) 
#                     norm_gt = self._skew_norm(slice_gt)
                    
#                     sample = np.column_stack([norm_in, slice_meal, slice_time, norm_gt]) # [Input, Sin, Cos, Truth]
#                     samples.append(sample)
                
#         return np.array(samples)

#     def _build_samples(self):
#         samples = []
#         for pid, group in self.df.groupby('pid'):
#             in_vals, gt_vals = group['cgm_simulated'].values, group['cgm'].values
#             meal_vals = np.where(group['meal'].values > 0, 1, 0)
            
#             dates = pd.to_datetime(group['date'])
#             time_indices = (dates.dt.hour * 60 + dates.dt.minute) // 5
#             time_indices = torch.tensor(time_indices.values)
#             time_embeds = self._absolute_time_encoding(time_indices).numpy()

#             num_points = len(group)
#             for i in range(0, num_points - self.seq_len + 1, self.stride):
#                 slice_gt = gt_vals[i : i + self.seq_len]
#                 if np.isnan(slice_gt).any(): continue

#                 slice_in = in_vals[i : i + self.seq_len]
#                 slice_meal = meal_vals[i : i + self.seq_len]
#                 slice_time = time_embeds[i : i + self.seq_len]
                
#                 # Normalize
#                 norm_in = self._skew_norm(slice_in) 
#                 norm_gt = self._skew_norm(slice_gt)
                
#                 sample = np.column_stack([norm_in, slice_meal, slice_time, norm_gt]) # [Input, Sin, Cos, Truth]
#                 samples.append(sample)
                
#         return np.array(samples)

    
#     def __len__(self):
#         return len(self.samples)
#     def __getitem__(self, idx):
#         return torch.FloatTensor(self.samples[idx])








# # import torch
# # import math
# # import numpy as np
# # import pandas as pd

# # def apply_mnar_values(drop_mask, day_df, max_points):
# #     """
# #     Simulates MNAR by masking ONLY contiguous Hyper/Hypo values.
# #     """
# #     cgm_values = day_df['cgm'].values
# #     is_extreme = (cgm_values > 150) | (cgm_values < 70)
    
# #     triggers = np.where(is_extreme)[0].tolist()
# #     np.random.shuffle(triggers)

# #     T = len(day_df)
# #     remaining = max_points - drop_mask.sum()
# #     if remaining <= 0: return drop_mask

# #     while remaining > 0 and len(triggers) > 0:
# #         loc_idx = triggers.pop()       
        
# #         if drop_mask[loc_idx]: continue
# #         L_max = np.random.randint(1, remaining + 1)        
# #         points_masked_in_this_loop = 0
        
# #         for i in range(L_max):
# #             curr = loc_idx + i
            
# #             if curr >= T: break
# #             if not is_extreme[curr]: break
# #             if drop_mask[curr]: break

# #             drop_mask[curr] = True
# #             points_masked_in_this_loop += 1
        
# #         remaining -= points_masked_in_this_loop

# #     return drop_mask

# # # def apply_mar_meal(drop_mask, day_df, max_points):
# # #     """ 
# # #     Meal simulation: Drops data segments starting at meal times.
# # #     """
# # #     triggers = np.where(day_df['meal'] != 0.0)[0]
# # #     np.random.shuffle(triggers)    
# # #     triggers = list(triggers) 

# # #     T = len(day_df)
# # #     remaining = max_points - drop_mask.sum()
# # #     if remaining <= 0: return drop_mask

# # #     while remaining > 0 and len(triggers) > 0:
# # #         start_idx = triggers.pop()
# # #         L = np.random.randint(1, remaining + 1)
        
# # #         if start_idx + L > T: continue
# # #         if drop_mask[start_idx : start_idx + L].any(): continue
# # #         drop_mask[start_idx : start_idx + L] = True
# # #         remaining -= L

# # #     return drop_mask




# # def apply_mar_meal(drop_mask, day_df, max_points):
# #     """ 
# #     HARD MODE MEAL SIMULATION
# #     """
# #     meal_indices = np.where(day_df['meal'] > 0)[0]
# #     if len(meal_indices) == 0: return drop_mask 

# #     largest_meal_idx = meal_indices[np.argmax(day_df.iloc[meal_indices]['meal'].values)]
# #     current_masked = drop_mask.sum()
# #     gap_points = max_points - current_masked
    
# #     if gap_points <= 0: return drop_mask

# #     start = largest_meal_idx
# #     end = min(start + gap_points, len(day_df))
    
# #     drop_mask[start:end] = True
    
# #     return drop_mask











# # def apply_mcar_random_noise(drop_mask, max_points):
# #     """
# #     MCAR with exact total missing = max_points
# #     """
# #     current = drop_mask.sum()
# #     remaining = max_points - current
# #     if remaining <= 0: return drop_mask

# #     safe_pos = np.where(~drop_mask)[0]
# #     remaining = min(remaining, len(safe_pos))

# #     if remaining > 0:
# #         mcar_pos = np.random.choice(safe_pos, size=remaining, replace=False)
# #         drop_mask[mcar_pos] = True

# #     return drop_mask


# # def process_single_day(day_df, max_pct, probs):
# #     df_day = day_df.copy()
# #     total_points = len(df_day)
# #     drop_mask = np.zeros(total_points, dtype=bool)
    
# #     max_points = int(total_points * max_pct)
    
# #     active_mnar     = np.random.rand() < probs.get('mnar', 0.5)
# #     active_meal     = np.random.rand() < probs.get('meal', 0.3)
# #     active_mcar     = np.random.rand() < probs.get('mcar', 0.8)
    
# #     if not (active_mnar or active_meal or active_mcar): return df_day

# #     # Apply Logic
# #     if active_mnar:
# #         drop_mask = apply_mnar_values(drop_mask, df_day, max_points)
# #     if active_meal:
# #         drop_mask = apply_mar_meal(drop_mask, df_day, max_points)
# #     if active_mcar:
# #         drop_mask = apply_mcar_random_noise(drop_mask, max_points)

# #     if drop_mask.sum() != max_points: return None
    
# #     col_loc = df_day.columns.get_loc('cgm_simulated')
# #     df_day.iloc[drop_mask, col_loc] = np.nan
# #     return df_day


# # def simulate_missingness_pipeline(df, max_pct, probs):
# #     df = df.copy()
# #     df['cgm_simulated'] = df['cgm'].copy()    
# #     return process_single_day(df, max_pct, probs)



# import numpy as np
# import pandas as pd

# POINTS_PER_HOUR = 12  
# MASK_WINDOW = int(1.5 * POINTS_PER_HOUR)  

# def apply_protocol_A_homeostatic(drop_mask, day_df):
#     """
#     Protocol A: The 'Steady-State' Mask (Homeostatic Regime).
#     Target: Windows defined by metabolic stability (Sleep or Fasting).
#     Condition: No meals (u_t = 0) and low variance.
#     Action: Mask 90-minute segments.
#     """
#     cgm_values = day_df['cgm'].values
#     meal_values = day_df['meal'].values
#     T = len(day_df)
    
#     valid_starts = []
    
#     gradients = np.abs(np.gradient(cgm_values))
#     is_stable = gradients < 2.0 
    
#     for t in range(T - MASK_WINDOW):
#         window_meal = meal_values[t : t + MASK_WINDOW]
#         window_stable = is_stable[t : t + MASK_WINDOW]
        
#         if (np.sum(window_meal) == 0) and (np.mean(window_stable) > 0.8):
#             valid_starts.append(t)
            
#     if not valid_starts: return drop_mask 
    
#     start_idx = np.random.choice(valid_starts)
#     drop_mask[start_idx : start_idx + MASK_WINDOW] = True
    
#     return drop_mask

# def apply_protocol_B_hidden_peak(drop_mask, day_df):
#     """
#     Protocol B: The 'Hidden Peak' Mask (Causal Regime).
#     Target: Post-prandial peaks.
#     Action: Mask 90-minute segments CENTERED on the peak following a meal.
#     """
#     cgm_values = day_df['cgm'].values
#     meal_indices = np.where(day_df['meal'] > 0)[0]
    
#     if len(meal_indices) == 0:
#         return drop_mask

#     meal_idx = np.random.choice(meal_indices)    
#     look_ahead = 24 
#     search_end = min(meal_idx + look_ahead, len(day_df))
    
#     if search_end <= meal_idx:
#         return drop_mask
        
#     local_window = cgm_values[meal_idx : search_end]
#     peak_offset = np.argmax(local_window)
#     peak_idx = meal_idx + peak_offset


#     MASK_WINDOW = 40
    
#     half_window = MASK_WINDOW // 2
#     start_idx = max(0, peak_idx - half_window)
#     end_idx = min(len(day_df), peak_idx + half_window)
    
#     if not drop_mask[start_idx:end_idx].any():
#         drop_mask[start_idx:end_idx] = True
        
#     return drop_mask

# def apply_protocol_C_hidden_rescue(drop_mask, day_df):
#     """
#     Protocol C: The 'Hidden Rescue' Mask (Hypo Detection).
#     Target: Nadir of hypoglycemic event (<70 mg/dL).
#     Action: Mask the drop/nadir to test if model infers rescue from suspension.
#     """
#     cgm_values = day_df['cgm'].values
    
#     hypo_indices = np.where(cgm_values < 70)[0]
    
#     if len(hypo_indices) == 0:
#         return drop_mask
        
#     nadir_idx = hypo_indices[np.argmin(cgm_values[hypo_indices])]
    
#     half_window = MASK_WINDOW // 2
#     start_idx = max(0, nadir_idx - half_window)
#     end_idx = min(len(day_df), nadir_idx + half_window)
    
#     drop_mask[start_idx:end_idx] = True
    
#     return drop_mask

# def process_single_day_experiment(day_df, experiment_mode='A'):
#     """
#     Master function to apply specific experimental protocols.
#     mode: 'A' (Homeostatic), 'B' (Hidden Peak), 'C' (Hidden Rescue), or 'Mixed'
#     """
#     df_day = day_df.copy()
#     total_points = len(df_day)
#     drop_mask = np.zeros(total_points, dtype=bool)
    
#     if experiment_mode == 'A':
#         drop_mask = apply_protocol_A_homeostatic(drop_mask, df_day)
#     elif experiment_mode == 'B':
#         drop_mask = apply_protocol_B_hidden_peak(drop_mask, df_day)
#     elif experiment_mode == 'C':
#         drop_mask = apply_protocol_C_hidden_rescue(drop_mask, df_day)
#     elif experiment_mode == 'Mixed':
#         if np.random.rand() < 0.33:
#             drop_mask = apply_protocol_C_hidden_rescue(drop_mask, df_day)
#         if np.random.rand() < 0.5: # Conditional on previous
#              drop_mask = apply_protocol_B_hidden_peak(drop_mask, df_day)
#         else:
#              drop_mask = apply_protocol_A_homeostatic(drop_mask, df_day)

#     # Apply NaNs
#     col_loc = df_day.columns.get_loc('cgm_simulated')
#     df_day.iloc[drop_mask, col_loc] = np.nan
    
#     return df_day

# def simulate_experiment_pipeline(df, mode='Mixed'):
#     df = df.copy()
#     if 'cgm_simulated' not in df.columns:
#         df['cgm_simulated'] = df['cgm'].copy()
#     return process_single_day_experiment(df, experiment_mode=mode)







# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# from scipy.stats import gamma
# from scipy.optimize import curve_fit

# class MISSINGNESS_PATTERN:
#     def __init__(self, datapath_list, target_col='Value'):
#         self.df = pd.concat([pd.read_csv(csv) for csv in datapath_list], axis=0, ignore_index=True)
        
#         self.df['DT_Index'] = pd.to_datetime(self.df['DT_Index'])
#         self.target_col = target_col
#         self.df[self.target_col] = pd.to_numeric(self.df[self.target_col], errors='coerce')
        
#     def resampling(self):
#         def enforce_5min(group):
#             group = group.drop_duplicates(subset='DT_Index')
#             group = group.set_index('DT_Index')
#             group = group.sort_index()

#             if group.empty: return group

#             start_time = group.index.min().floor('D')
#             end_time = group.index.max().ceil('D') - pd.Timedelta(minutes=5)
#             full_grid = pd.date_range(start=start_time, end=end_time, freq='5min', name='DT_Index')
            
#             return group.reindex(full_grid)

#         df_regular = self.df.groupby('SID').apply(enforce_5min)        
        
#         if 'SID' in df_regular.columns: df_regular = df_regular.drop(columns=['SID'])
#         df_regular = df_regular.reset_index()
#         if 'DT_Index' not in df_regular.columns and 'level_1' in df_regular.columns:
#             df_regular = df_regular.rename(columns={'level_1': 'DT_Index'})
            
#         return df_regular

#     def filter_valid_days(self, df, threshold=0.50):
#         """
#         Removes 'Ghost Days' (low adherence) so they don't corrupt the gap analysis.
#         Standard requirement: >70% data (approx 200/288 points).
#         """
#         expected_points = 288
#         min_required = expected_points * threshold
        
#         df_clean = df.copy()
#         df_clean['Date'] = df_clean['DT_Index'].dt.date
        
#         daily_counts = df_clean.groupby(['SID', 'Date'])[self.target_col].count()
#         valid_days = daily_counts[daily_counts >= min_required].index
        
#         df_clean = df_clean.set_index(['SID', 'Date'])
#         df_filtered = df_clean.loc[df_clean.index.isin(valid_days)].reset_index()
        
#         return df_filtered

#     def analyze_gaps(self, rs_df):
#         gap_data = []
#         for sid, grp in rs_df.groupby('SID'):
#             mask = grp[self.target_col].isna()
#             blocks = (mask != mask.shift()).cumsum()
            
#             gap_summary = grp[mask].groupby(blocks).agg(
#                 Start_Time=('DT_Index', 'first'),
#                 Count=('DT_Index', 'size')
#             )
            
#             gap_summary['Duration_Min'] = gap_summary['Count'] * 5
#             gap_summary['SID'] = sid
#             gap_summary['Hour_of_Day'] = gap_summary['Start_Time'].dt.hour
#             gap_summary['Date'] = gap_summary['Start_Time'].dt.date
            
#             gap_data.append(gap_summary)

#         if gap_data:
#             return pd.concat(gap_data).reset_index(drop=True)
#         else:
#             return pd.DataFrame()

#     def analyze_hourly_profile(self, gap_stats):
#         """
#         Calculates the 'When': Probability of a gap starting at Hour X.
#         """
#         counts = gap_stats['Hour_of_Day'].value_counts().sort_index()
#         probs = counts / counts.sum()
#         return probs.reindex(range(24), fill_value=0)


#     def fit_global_distribution(self, gaps):
#         """
#         Fits the distribution in two parts:
#         1. Exact probability of 'Single Drops' (5 min).
#         2. Mixture Model (Exp + Gauss + Uniform Offset) for 'Real Gaps' (>5 min).
#         """
#         all_data = gaps['Duration_Min'].values
        
#         n_total = len(all_data)
#         n_single = np.sum(all_data <= 5)
#         prob_single = n_single / n_total if n_total > 0 else 0
#         tail_data = all_data[all_data > 5]
        
#         if len(tail_data) == 0: return prob_single, None

#         counts, bin_edges = np.histogram(tail_data, bins=47, density=True)
#         bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
        
#         def mixture_func(x, A, k, B, mu, sigma, C):
#             exp_part = A * np.exp(-k * x)
#             gauss_part = B * np.exp(-((x - mu)**2) / (2 * sigma**2))
#             return exp_part + gauss_part + C

#         p0 = [np.max(counts), 0.05, np.max(counts)/10, 120, 20, 0.005]
        
#         bounds = ([0, 0.001, 0, 60, 5, 0], [np.inf, 0.2, np.inf, 240, 60, 0.01])

#         try:
#             mix_params, _ = curve_fit(mixture_func, bin_centers, counts, 
#                                     p0=p0, bounds=bounds, maxfev=10000)
#             print("Tail Mixture Model (with Offset) Fitted Successfully.")
#         except RuntimeError:
#             print("Mixture fit failed.")
#             mix_params = None

#         return prob_single, mix_params


# class RealisticMaskGenerator:
#     def __init__(self, hourly_rate, prob_single, mix_params):
#         self.hourly_probs = hourly_rate.values if hasattr(hourly_rate, 'values') else hourly_rate
#         self.prob_single = prob_single
#         self.use_mixture = (mix_params is not None)
        
#         if self.use_mixture:
#             self.A, self.k, self.B, self.mu, self.sigma, self.C = mix_params
#             area_exp = (self.A / self.k) * np.exp(-self.k * 5)            
#             area_gauss = self.B * self.sigma * np.sqrt(2 * np.pi)            
#             area_unif = self.C * (240 - 5)
            
#             total_area = area_exp + area_gauss + area_unif
            
#             self.p_exp = area_exp / total_area
#             self.p_gauss = area_gauss / total_area

#     def sample_duration(self):
#         """Decides 'How Long' using the Two-Stage logic."""
#         if np.random.rand() < self.prob_single: return 5.0
#         if self.use_mixture:
#             r = np.random.rand()
#             if r < self.p_exp:
#                 duration = np.random.exponential(scale=(1 / self.k)) + 5
#             elif r < (self.p_exp + self.p_gauss):
#                 duration = np.random.normal(loc=self.mu, scale=self.sigma)                
#             else:
#                 duration = np.random.uniform(10, 240)
#             return max(10, min(duration, 1440))
#         return 10.0

#     def generate_mask(self, df_slice, points_per_hour=12):
#         """Creates a boolean mask (0=Missing, 1=Observed)."""  
#         df_slice = df_slice.copy()
#         if 'cgm_simulated' not in df_slice.columns: df_slice['cgm_simulated'] = df_slice['cgm'].copy()
#         dt_series = pd.to_datetime(df_slice['date'])
#         hour_list = dt_series.dt.hour.unique()
#         start_minute = dt_series.dt.minute.iloc[0]        
#         total_points = len(df_slice)
#         drop_mask = np.zeros(total_points, dtype=bool)
        
#         for i, hour in enumerate(hour_list):
#             if np.random.rand() < self.hourly_probs[hour]:
#                 duration_mins = self.sample_duration()
#                 duration_points = int(round(duration_mins / 5))
                
#                 if duration_points > 0:
#                     low_limit = int(start_minute / 5) if i == 0 else 0                    
#                     if low_limit >= points_per_hour: continue

#                     start_offset = np.random.randint(low_limit, points_per_hour)
#                     hour_indices = np.where(dt_series.dt.hour == hour)[0]
                    
#                     if len(hour_indices) > 0:
#                         base_idx = hour_indices[0]
#                         abs_start = base_idx + start_offset
#                         abs_end = min(abs_start + duration_points, total_points)
    
#                         drop_mask[abs_start : abs_end] = True

#         if 'cgm_simulated' in df_slice.columns: df_slice.loc[drop_mask, 'cgm_simulated'] = np.nan  
#         return df_slice


# def init_train_masking(csv_list, threshold):
#     """ Init training real-world masking. """
    
#     pipeline = MISSINGNESS_PATTERN(csv_list)
#     df_resampled = pipeline.resampling()
#     df_clean = pipeline.filter_valid_days(df_resampled, threshold=threshold)
#     gaps = pipeline.analyze_gaps(df_clean)
#     hourly_rate = pipeline.analyze_hourly_profile(gaps)
    
#     prob_single, mix_p = pipeline.fit_global_distribution(gaps)
    
#     gen = RealisticMaskGenerator(hourly_rate, prob_single, mix_p)
#     return gen






# def apply_protocol_B_hidden_peak(drop_mask, day_df, missing_config, POINTS_PER_HOUR=12):
#     """
#     Protocol B: Hidden Peak Mask.
#     Masks post-prandial peaks with RANDOM window lengths centered on the peak.
#     """
#     T, cgm_values = len(day_df), day_df['cgm'].values
    
#     MIN_WINDOW_LEN, MAX_WINDOW_LEN = int(2.5 * POINTS_PER_HOUR), int(3.0 * POINTS_PER_HOUR) 
#     min_total_points, max_total_points = int(T * missing_config['min']), int(T * missing_config['max'])
#     target_total_points = np.random.randint(min_total_points, max_total_points + 1)

#     peak_candidates = []
#     for meal_idx in np.where(day_df['meal'] > 0)[0]:
#         search_end = min(meal_idx + 24, T)
#         if search_end > meal_idx: peak_candidates.append(meal_idx + np.argmax(cgm_values[meal_idx : search_end]))

#     if not peak_candidates: return drop_mask
#     points_added = 0
#     np.random.shuffle(peak_candidates)
#     for peak_idx in peak_candidates:
#         if points_added >= target_total_points: break

#         current_window_len = np.random.randint(MIN_WINDOW_LEN, MAX_WINDOW_LEN + 1)
#         start_idx = max(0, peak_idx - (current_window_len // 2))
#         end_idx = min(T, start_idx + current_window_len)
#         actual_len = end_idx - start_idx

#         if not drop_mask[start_idx : end_idx].any():
#             if (points_added + actual_len) <= target_total_points:
#                 drop_mask[start_idx : end_idx] = True
#                 points_added += actual_len

#     return drop_mask


# def apply_protocol_A_homeostatic(drop_mask, day_df, missing_config, POINTS_PER_HOUR=12):
#     """
#     Protocol A: Steady-State Mask.
#     Fills stable areas with RANDOM window lengths (within a range)
#     until the Total Target is met.
#     """
#     T = len(day_df)
#     cgm_values, meal_values, bolus_values = day_df['cgm'].values, day_df['meal'].values, day_df['bolus'].values
    
#     MIN_WINDOW_LEN, MAX_WINDOW_LEN = int(1.0 * POINTS_PER_HOUR), int(2.0 * POINTS_PER_HOUR) 
#     min_total_points, max_total_points = int(T * missing_config['min']), int(T * missing_config['max'])
#     target_total_points = np.random.randint(min_total_points, max_total_points + 1)

#     valid_starts = []
#     gradients = np.abs(np.gradient(cgm_values))
#     is_stable = gradients < 2.0 
    
#     for t in range(T - MAX_WINDOW_LEN):
#         window_meal = meal_values[t : t + MAX_WINDOW_LEN]
#         window_bolus = bolus_values[t : t + MAX_WINDOW_LEN]
#         window_stable = is_stable[t : t + MAX_WINDOW_LEN]
#         if ((np.sum(window_meal) == 0) and (np.sum(window_bolus) == 0)) and (np.mean(window_stable) > 0.8): valid_starts.append(t)
            
#     if not valid_starts: return drop_mask 
#     points_added = 0
#     np.random.shuffle(valid_starts)
#     for start_idx in valid_starts:
#         if points_added >= target_total_points: break
#         current_window_len = np.random.randint(MIN_WINDOW_LEN, MAX_WINDOW_LEN + 1)        
#         end_idx = start_idx + current_window_len
        
#         if not drop_mask[start_idx : end_idx].any():
#             if (points_added + current_window_len) <= target_total_points:
#                 drop_mask[start_idx : end_idx] = True
#                 points_added += current_window_len
                
#     return drop_mask






# def process_single_day_experiment(day_df, experiment_mode='A', protocol_mask_ratio=0.1):
#     """
#     Master function to apply specific experimental protocols.
#     """
#     df_day = day_df.copy()
#     total_points = len(df_day)
#     drop_mask = np.zeros(total_points, dtype=bool)
    
#     config_A = {'min': protocol_mask_ratio, 'max': protocol_mask_ratio}
#     config_B = {'min': protocol_mask_ratio, 'max': protocol_mask_ratio}
    
#     if experiment_mode == 'A':
#         drop_mask = apply_protocol_A_homeostatic(drop_mask, df_day, config_A)
#     elif experiment_mode == 'B':
#         drop_mask = apply_protocol_B_hidden_peak(drop_mask, df_day, config_B)
#     elif experiment_mode == 'C':
#         drop_mask = apply_protocol_C_TCR(drop_mask, df_day)
#     elif experiment_mode == 'Mixed':
#         choice_prob = np.random.rand()
#         if choice_prob < 0.2:
#             drop_mask = apply_protocol_A_homeostatic(drop_mask, df_day, config_A)
#         elif choice_prob < 0.5:
#             drop_mask = apply_protocol_B_hidden_peak(drop_mask, df_day, config_B)
#         else:
#             drop_mask = apply_protocol_A_homeostatic(drop_mask, df_day, config_A)
#             drop_mask = apply_protocol_B_hidden_peak(drop_mask, df_day, config_B)
            
#     if 'cgm_simulated' in df_day.columns:
#         col_loc = df_day.columns.get_loc('cgm_simulated')
#         df_day.iloc[drop_mask, col_loc] = np.nan
    
#     return df_day




import numpy as np
import pandas as pd
from .protocol_B import apply_protocol_B_hidden_peak_engine
from .protocol_A import apply_protocol_A_homestatic_engine

# def apply_protocol_A_homeostatic(drop_mask, day_df, missing_config, BASE_WINDOW_LEN=6):
#     """
#     Protocol A: Steady-State Mask.
#     Fills stable areas with fixed window lengths.
#     """
#     temp_mask = drop_mask.copy() 
#     points_added, valid_starts, T = 0, [], len(day_df)
    
#     cgm_values, meal_values, bolus_values = day_df['cgm'].values, day_df['meal'].values, day_df['bolus'].values    
#     target_total_points = np.random.randint(int(T * missing_config['min']), int(T * missing_config['max']) + 1)

#     gradients = np.abs(np.gradient(cgm_values))
#     is_stable = gradients < 2.5
    
#     for t in range(T - BASE_WINDOW_LEN + 1):
#         window_meal = meal_values[t : t + BASE_WINDOW_LEN]
#         window_bolus = bolus_values[t : t + BASE_WINDOW_LEN]
#         window_stable = is_stable[t : t + BASE_WINDOW_LEN]
        
#         if ((np.sum(window_meal) == 0) and (np.sum(window_bolus) == 0) and (np.mean(window_stable) > 0.8)): valid_starts.append(t)

#     if len(valid_starts) * BASE_WINDOW_LEN < target_total_points: return drop_mask

#     np.random.shuffle(valid_starts)    
#     for start_idx in valid_starts:
#         if points_added >= target_total_points: break

#         points_remaining = target_total_points - points_added        
#         current_len = min(BASE_WINDOW_LEN, points_remaining)
#         end_idx = start_idx + current_len

#         if not temp_mask[start_idx : end_idx].any():
#             temp_mask[start_idx : end_idx] = True
#             points_added += current_len
#     if points_added < target_total_points: return drop_mask
#     return temp_mask


def apply_protocol_A_homeostatic(drop_mask, day_df, missing_config, BASE_WINDOW_LEN=6, LOOKBACK=12):
    """
    Protocol A: Steady-State Mask.
    Fills stable areas with fixed window lengths, avoiding excursion periods.
    """
    drop_mask = apply_protocol_A_homestatic_engine(drop_mask, day_df, missing_config, BASE_WINDOW_LEN=6, LOOKBACK=12)
    return drop_mask

def apply_protocol_B_hidden_peak(drop_mask, day_df, missing_config, POINTS_PER_HOUR=12):
    """
    Protocol B: Hidden Peak Mask.
    Masks post-prandial peaks centered on the peak location.
    """
    drop_mask = apply_protocol_B_hidden_peak_engine(drop_mask, day_df, missing_config)
    return drop_mask

# def apply_protocol_C_TCR(drop_mask, day_df):
#     """
#     Protocol C: Masks during TCR activations if hypoglycemia occurs.
#     """

#     cgm = day_df['cgm'].values
#     tcr_flag = day_df['tcr_flag'].values.astype('bool') 
#     if (cgm[tcr_flag] < 70).any(): drop_mask = tcr_flag
#     print(drop_mask)
#     return drop_mask



def apply_protocol_C_TCR(drop_mask, day_df):
    """
    Protocol C: Masks 1-hour window around hypoglycemia during TCR activations.
    """
    cgm = day_df['cgm'].values
    tcr_flag = day_df['tcr_flag'].values.astype('bool')
    
    low_during_tcr = tcr_flag & (cgm < 70)
    
    if low_during_tcr.any():
        low_indices = np.where(low_during_tcr)[0]
        window_size = 6 
        
        for idx in low_indices:
            start = max(0, idx - window_size)
            end = min(len(cgm), idx + window_size + 1)
            drop_mask[start:end] = True
    
    return drop_mask




def process_single_day_experiment(day_df, miss_config):
    """
    Master function to apply specific experimental protocols.
    """
    experiment_mode, protocol_mask_ratio =  miss_config['type'], miss_config['protocol_mask_ratio']
    df_day = day_df.copy()
    total_points = len(df_day)
    drop_mask = np.zeros(total_points, dtype=bool)
    
    config_A = {'min': protocol_mask_ratio, 'max': protocol_mask_ratio}
    
    if experiment_mode == 'A':
        drop_mask = apply_protocol_A_homeostatic(drop_mask, df_day, config_A)
    elif experiment_mode == 'B':
        drop_mask = apply_protocol_B_hidden_peak(drop_mask, df_day, miss_config)
    elif experiment_mode == 'C':
        drop_mask = apply_protocol_C_TCR(drop_mask, df_day)
    elif experiment_mode == 'Mixed':
        choice_prob = np.random.rand()
        if choice_prob < 0.2:
            drop_mask = apply_protocol_A_homeostatic(drop_mask, df_day, config_A)
        elif choice_prob < 0.5:
            drop_mask = apply_protocol_B_hidden_peak(drop_mask, df_day, miss_config)
        else:
            drop_mask = apply_protocol_A_homeostatic(drop_mask, df_day, config_A)
            drop_mask = apply_protocol_B_hidden_peak(drop_mask, df_day, miss_config)
            
    if 'cgm_simulated' in df_day.columns:
        col_loc = df_day.columns.get_loc('cgm_simulated')
        df_day.iloc[drop_mask, col_loc] = np.nan
    
    return df_day



def simulate_experiment_pipeline(df, miss_config):
    df = df.copy()
    if 'cgm_simulated' not in df.columns:
        df['cgm_simulated'] = df['cgm'].copy()
    return process_single_day_experiment(df, miss_config)