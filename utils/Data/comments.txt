# import numpy as np
# from pygrinder import mcar, seq_missing, block_missing

# def create_missing(cgm, rate, pattern='mcar', **kwargs):
#     """ Create missingness based on patterns. """
#     assert pattern in ['mcar', 'seq_missing', 'block_missing'], 'pattern should be mcar, seq_missing or block_missing.'
#     if pattern == 'mcar':
#         return mcar(cgm, rate)
#     elif pattern == 'seq_missing':
#         return seq_missing(np.expand_dims(cgm, axis=0), rate, seq_len=kwargs['seq_len']).squeeze(0)
#     else:
#         return block_missing(np.expand_dims(cgm, axis=0), factor=rate, block_len=kwargs['block_len'], block_width=kwargs['block_width']).squeeze(0)
        





# # import torch, math
# # import numpy as np
# # import pandas as pd
# # from .missing import create_missing

# # class CGMDataset:
# #     """ CGM Dataset preparation. """
# #     def __init__(self, seq_len, data_dir, miss_cfg, stride=12, data_cols=['cgm'], unique_col='pid', missing_enabled=False):
# #         self.seq_len = seq_len
# #         self.stride = stride
# #         self.data_dir = data_dir
# #         self.data_cols = data_cols
# #         self.unique_col = unique_col
# #         self.miss_cfg = miss_cfg
# #         self.missing_enabled = missing_enabled
       
# #     def load_data(self):
# #         df = pd.read_csv(self.data_dir)
# #         return df if set(self.data_cols + [self.unique_col]).issubset(df.columns) else None
   
# #     def get_numpy_arrays(self, df):        
# #         data, pid = [df[col].values for col in self.data_cols], df[self.unique_col].values
# #         return data, pid

# #     def valid_pid(self, pid, start, length):
# #         return (pid[start:start + length] == pid[start]).all()

# #     def has_nan(self, arrays, start, length):
# #         return any(np.isnan(arr[start:start + length]).any() for arr in arrays)

# #     def get_input_slice(self, arrays, start):
# #         return np.stack([arr[start:start + self.seq_len] for arr in arrays], axis=1)

# #     def skew_norm(self, x, skew, min_val, max_val):
# #         """Normalize the input x to the range [0, 1] using a skewed function."""
# #         x = np.array(x) # Ensure input is numpy array
# #         if x.ndim == 1:
# #             x = np.expand_dims(x, 0)
# #         x = np.maximum(np.minimum(x, max_val), min_val)
# #         return np.power((x - min_val) / (max_val - min_val), skew)
    
# #     def build_dataset(self):
# #         df = self.load_data()
# #         if df is None: return None, 0
# #         data, pid = self.get_numpy_arrays(df)
# #         x_list = []
        
# #         for day_start in range(0, len(df) - 288 + 1, 288):
# #             for i in range(day_start, day_start + 288 - self.seq_len + 1, self.stride):
# #                 if not self.valid_pid(pid, i, self.seq_len): continue
# #                 if self.has_nan(data, i, self.seq_len): continue
    
# #                 x = self.get_input_slice(data, i)
# #                 x = self.skew_norm(x, 1, 40, 400)
# #                 if self.missing_enabled:
# #                     x = create_missing(x, self.miss_cfg["rate"], self.miss_cfg["pattern"], **{k: v for k, v in self.miss_cfg.items() if k not in ("rate", "pattern")})
# #                 x_list.append(x)
    
# #         x_samples = np.stack(x_list, axis=0).astype(float)
# #         return x_samples, x_samples.shape[0]







# import torch, math
# import numpy as np
# import pandas as pd
# from torch.utils.data import Dataset
# from .missing import simulate_missingness_pipeline
# from .test_missing import simulate_missingness_test_pipeline

# class CGMDataset(Dataset):
#     def __init__(self, data_path, seq_len=288, stride=12, 
#                  missing_enabled=True, miss_cfg=None, is_evaluate=False):
#         self.seq_len = seq_len
#         self.stride = stride
#         self.missing_enabled = missing_enabled
#         self.miss_cfg = miss_cfg or {}
        
#         self.df = pd.read_csv(data_path)
        
#         if self.missing_enabled:
#             if not is_evaluate:
#                 self.df = simulate_missingness_pipeline(
#                     self.df,
#                     min_pct=self.miss_cfg.get('min_pct', 0.1),
#                     max_pct=self.miss_cfg.get('max_pct', 0.3),
#                     probs=self.miss_cfg.get('probs', {'mnar': 0.5, 'pisa': 0.3, 'mcar': 0.8}),
#                 )
#             else:
#                 self.df = simulate_missingness_test_pipeline(
#                     self.df,
#                     max_pct=self.miss_cfg.get('max_pct', 0.3),
#                     probs=self.miss_cfg.get('probs', {'mnar': 0.5, 'pisa': 0.3, 'mcar': 0.8}),
#                 )                
#         else:
#             self.df['cgm_simulated'] = self.df['cgm'].copy() 

#         self.samples = self._build_samples()

#     def _absolute_time_encoding(self, indices, T_day=288):
#         t = indices.float() / float(T_day)
#         return torch.stack([torch.sin(2 * math.pi * t), torch.cos(2 * math.pi * t)], dim=-1)

#     def _skew_norm(self, x, skew=1, min_val=40, max_val=400):
#         x = np.array(x)
#         x = np.maximum(np.minimum(x, max_val), min_val)
#         return np.power((x - min_val) / (max_val - min_val), skew)

#     def _build_samples(self):
#         samples = []
#         for pid, group in self.df.groupby('pid'):
#             in_vals, gt_vals = group['cgm_simulated'].values, group['cgm'].values
            
#             dates = pd.to_datetime(group['date'])
#             time_indices = (dates.dt.hour * 60 + dates.dt.minute) // 5
#             time_indices = torch.tensor(time_indices.values)
#             time_embeds = self._absolute_time_encoding(time_indices).numpy()

#             num_points = len(group)
#             for i in range(0, num_points - self.seq_len + 1, self.stride):
#                 slice_gt = gt_vals[i : i + self.seq_len]
#                 if np.isnan(slice_gt).any(): continue

#                 slice_in = in_vals[i : i + self.seq_len]
#                 slice_time = time_embeds[i : i + self.seq_len]
                
#                 # Normalize
#                 norm_in = self._skew_norm(slice_in) 
#                 norm_gt = self._skew_norm(slice_gt)
                
#                 sample = np.column_stack([norm_in, slice_time, norm_gt]) # [Input, Sin, Cos, Truth]
#                 samples.append(sample)
                
#         return np.array(samples)

#     def __len__(self):
#         return len(self.samples)
#     def __getitem__(self, idx):
#         return torch.FloatTensor(self.samples[idx])




# import torch, math
# import numpy as np
# import pandas as pd
# from torch.utils.data import Dataset
# from .missing import simulate_missingness_pipeline
# from .test_missing import simulate_missingness_test_pipeline

# class CGMDataset(Dataset):
#     def __init__(self, data_path, seq_len=288, stride=12, 
#                  missing_enabled=True, miss_cfg=None, is_evaluate=False, is_dclp3=False):
#         self.seq_len = seq_len
#         self.stride = stride
#         self.missing_enabled = missing_enabled
#         self.miss_cfg = miss_cfg or {}
        
#         self.df = pd.read_csv(data_path)
        
#         if self.missing_enabled:
#             if not is_evaluate:
#                 self.df = simulate_missingness_pipeline(
#                     self.df,
#                     min_pct=self.miss_cfg.get('min_pct', 0.1),
#                     max_pct=self.miss_cfg.get('max_pct', 0.3),
#                     probs=self.miss_cfg.get('probs', {'mnar': 0.5, 'pisa': 0.3, 'mcar': 0.8}),
#                 )
#             else:
#                 self.df = simulate_missingness_test_pipeline(
#                     self.df,
#                     max_pct=self.miss_cfg.get('max_pct', 0.3),
#                     probs=self.miss_cfg.get('probs', {'mnar': 0.5, 'pisa': 0.3, 'mcar': 0.8}),
#                 )                
#         else:
#             self.df['cgm_simulated'] = self.df['cgm'].copy() 

#         self.samples = self._build_samples_dclp3() if is_dclp3 else self._build_samples()

#     def _absolute_time_encoding(self, indices, T_day=288):
#         t = indices.float() / float(T_day)
#         return torch.stack([torch.sin(2 * math.pi * t), torch.cos(2 * math.pi * t)], dim=-1)

#     def _skew_norm(self, x, skew=1, min_val=40, max_val=400):
#         x = np.array(x)
#         x = np.maximum(np.minimum(x, max_val), min_val)
#         return np.power((x - min_val) / (max_val - min_val), skew)

#     def _build_samples_dclp3(self):
#         samples = []
#         for pid, group in self.df.groupby('pat_id'):
#             for seq_id, episode in group.groupby('seq_id'):
#                 in_vals, gt_vals = episode['cgm_simulated'].values, episode['cgm'].values
                
#                 dates = pd.to_datetime(episode['date'])
#                 time_indices = (dates.dt.hour * 60 + dates.dt.minute) // 5
#                 time_indices = torch.tensor(time_indices.values)
#                 time_embeds = self._absolute_time_encoding(time_indices).numpy()
    
#                 num_points = len(episode)
#                 for i in range(0, num_points - self.seq_len + 1, self.stride):
#                     slice_gt = gt_vals[i : i + self.seq_len]
#                     if np.isnan(slice_gt).any(): continue
    
#                     slice_in = in_vals[i : i + self.seq_len]
#                     slice_time = time_embeds[i : i + self.seq_len]
                    
#                     # Normalize
#                     norm_in = self._skew_norm(slice_in) 
#                     norm_gt = self._skew_norm(slice_gt)
                    
#                     sample = np.column_stack([norm_in, slice_time, norm_gt]) # [Input, Sin, Cos, Truth]
#                     samples.append(sample)
                
#         return np.array(samples)

#     def _build_samples(self):
#         samples = []
#         for pid, group in self.df.groupby('pid'):
#             in_vals, gt_vals = group['cgm_simulated'].values, group['cgm'].values
            
#             dates = pd.to_datetime(group['date'])
#             time_indices = (dates.dt.hour * 60 + dates.dt.minute) // 5
#             time_indices = torch.tensor(time_indices.values)
#             time_embeds = self._absolute_time_encoding(time_indices).numpy()

#             num_points = len(group)
#             for i in range(0, num_points - self.seq_len + 1, self.stride):
#                 slice_gt = gt_vals[i : i + self.seq_len]
#                 if np.isnan(slice_gt).any(): continue

#                 slice_in = in_vals[i : i + self.seq_len]
#                 slice_time = time_embeds[i : i + self.seq_len]
                
#                 # Normalize
#                 norm_in = self._skew_norm(slice_in) 
#                 norm_gt = self._skew_norm(slice_gt)
                
#                 sample = np.column_stack([norm_in, slice_time, norm_gt]) # [Input, Sin, Cos, Truth]
#                 samples.append(sample)
                
#         return np.array(samples)

    
#     def __len__(self):
#         return len(self.samples)
#     def __getitem__(self, idx):
#         return torch.FloatTensor(self.samples[idx])






# import torch
# import math
# import numpy as np
# import pandas as pd


# def apply_mnar_values(drop_mask, day_df, max_points):
#     """
#     Simulates NMAR (Not Missing At Random) by masking values based on their 
#     actual glucose level (High > 150 or Low < 70).
#     """
#     #    Thresholds: < 70 mg/dL or > 150 mg/dL 
#     triggers = day_df[(day_df['cgm'] > 150) | (day_df['cgm'] < 70)].index.tolist()
#     np.random.shuffle(triggers)
    
#     for trigger_idx in triggers:
#         if drop_mask.sum() >= max_points:
#             break
            
#         loc_idx = day_df.index.get_loc(trigger_idx)
#         duration = int(np.random.randint(30, 90) / 5)
        
#         # Calculate remaining quota to avoid over-masking
#         remaining = max_points - drop_mask.sum()
#         if duration > remaining:
#             duration = remaining
            
#         if duration > 0:
#             end_loc = min(loc_idx + duration, len(day_df))
#             drop_mask[loc_idx : end_loc] = True
            
#     return drop_mask


# def apply_mar_pisa(drop_mask, day_df, max_points):
#     """ PISA simulation. """
#     remaining = max_points - drop_mask.sum()
#     if remaining <= 0: return drop_mask

#     # Define sleep mask
#     is_sleep = (day_df['date'].dt.hour <= 7) | (day_df['date'].dt.hour >= 23)
#     sleep_pos = np.where(is_sleep)[0]
    
#     if len(sleep_pos) > 0:
#         pisa_start = np.random.choice(sleep_pos)
#         duration = int(np.random.randint(20, 60) / 5)
        
#         if duration > remaining: duration = remaining
        
#         if duration > 0:
#             end_loc = min(pisa_start + duration, len(day_df))
#             intended_window = is_sleep.iloc[pisa_start : end_loc]
            
#             if (~intended_window).any():
#                 first_wake_idx = np.where(~intended_window)[0][0]
#                 end_loc = pisa_start + first_wake_idx
            
#             # Apply mask
#             drop_mask[pisa_start : end_loc] = True
            
#     return drop_mask


# def apply_mcar_random_noise(drop_mask, min_points, max_points):
#     """ Randomly Noises. """
#     current_drops = drop_mask.sum()
#     needed = 0
    
#     if current_drops < min_points:
#         needed = min_points - current_drops
#     elif current_drops < max_points:
#         remaining = max_points - current_drops
#         needed = int(np.random.rand() * remaining * 0.2)
        
#     if needed > 0:
#         safe_pos = np.where(drop_mask == False)[0]
#         needed = min(needed, len(safe_pos))
#         if needed > 0:
#             mcar_pos = np.random.choice(safe_pos, size=needed, replace=False)
#             drop_mask[mcar_pos] = True
#     return drop_mask

# def process_single_day(day_df, min_pct, max_pct, probs):
#     df_day = day_df.copy()
#     total_points = len(df_day)
#     drop_mask = np.zeros(total_points, dtype=bool)
    
#     max_points = int(total_points * max_pct)
#     min_points = int(total_points * min_pct)
    
#     active_mnar     = np.random.rand() < probs.get('mnar', 0.5)
#     active_pisa     = np.random.rand() < probs.get('pisa', 0.3)
#     active_mcar     = np.random.rand() < probs.get('mcar', 0.8)
    
#     if not (active_mnar or active_pisa or active_mcar):
#         return df_day
#     # Apply Logic
#     if active_mnar:
#         drop_mask = apply_mnar_values(drop_mask, df_day, max_points)
#     if active_pisa:
#         drop_mask = apply_mar_pisa(drop_mask, df_day, max_points)
#     if active_mcar:
#         drop_mask = apply_mcar_random_noise(drop_mask, min_points, max_points)

#     col_loc = df_day.columns.get_loc('cgm_simulated')
#     df_day.iloc[drop_mask, col_loc] = np.nan
#     return df_day

# def simulate_missingness_pipeline(df, min_pct, max_pct, probs):
#     df = df.copy()
#     if not pd.api.types.is_datetime64_any_dtype(df['date']):
#         df['date'] = pd.to_datetime(df['date'])
    
#     df['cgm_simulated'] = df['cgm'].copy()
    
#     def process_patient(patient_df):
#         day_groups = patient_df.groupby(patient_df['date'].dt.date)
#         results = []
#         for date, day_data in day_groups:
#             processed_day = process_single_day(day_data, min_pct, max_pct, probs)
#             results.append(processed_day)
#         return pd.concat(results)

#     if 'pid' in df.columns:
#         final_df = df.groupby('pid').apply(process_patient).reset_index(drop=True)
#     else:
#         final_df = process_patient(df)
#     return final_df





# import torch, math
# import numpy as np
# import pandas as pd
# from torch.utils.data import Dataset
# from .missing import simulate_missingness_pipeline

# class CGMDataset(Dataset):
#     def __init__(self, data_path, seq_len=288, stride=12, 
#                  missing_enabled=True, miss_cfg=None, is_dclp3=False):
#         self.seq_len = seq_len
#         self.stride = stride
#         self.missing_enabled = missing_enabled
#         self.miss_cfg = miss_cfg or {}
        
#         self.df = pd.read_csv(data_path)
        
#         if self.missing_enabled:
#             self.df = simulate_missingness_pipeline(
#                 self.df,
#                 max_pct=self.miss_cfg.get('max_pct', 0.3),
#                 probs=self.miss_cfg.get('probs', {'mnar': 0.5, 'meal': 0.3, 'mcar': 0.8}),
#             )                
#         else:
#             self.df['cgm_simulated'] = self.df['cgm'].copy() 

#         self.samples = self._build_samples_dclp3() if is_dclp3 else self._build_samples()

#     def _absolute_time_encoding(self, indices, T_day=288):
#         t = indices.float() / float(T_day)
#         return torch.stack([torch.sin(2 * math.pi * t), torch.cos(2 * math.pi * t)], dim=-1)

#     def _skew_norm(self, x, skew=1, min_val=40, max_val=400):
#         x = np.array(x)
#         x = np.maximum(np.minimum(x, max_val), min_val)
#         return np.power((x - min_val) / (max_val - min_val), skew)

#     def _build_samples_dclp3(self):
#         samples = []
#         for pid, group in self.df.groupby('pat_id'):
#             for seq_id, episode in group.groupby('seq_id'):
#                 in_vals, gt_vals = episode['cgm_simulated'].values, episode['cgm'].values
#                 meal_vals = np.where(episode['meal'].values > 0, 1, 0)
                
#                 dates = pd.to_datetime(episode['date'])
#                 time_indices = (dates.dt.hour * 60 + dates.dt.minute) // 5
#                 time_indices = torch.tensor(time_indices.values)
#                 time_embeds = self._absolute_time_encoding(time_indices).numpy()
    
#                 num_points = len(episode)
#                 for i in range(0, num_points - self.seq_len + 1, self.stride):
#                     slice_gt = gt_vals[i : i + self.seq_len]
#                     if np.isnan(slice_gt).any(): continue
    
#                     slice_in = in_vals[i : i + self.seq_len]
#                     slice_meal = meal_vals[i : i + self.seq_len]
#                     slice_time = time_embeds[i : i + self.seq_len]
                    
#                     # Normalize
#                     norm_in = self._skew_norm(slice_in) 
#                     norm_gt = self._skew_norm(slice_gt)
                    
#                     sample = np.column_stack([norm_in, slice_meal, slice_time, norm_gt]) # [Input, Sin, Cos, Truth]
#                     samples.append(sample)
                
#         return np.array(samples)

#     def _build_samples(self):
#         samples = []
#         for pid, group in self.df.groupby('pid'):
#             in_vals, gt_vals = group['cgm_simulated'].values, group['cgm'].values
#             meal_vals = np.where(group['meal'].values > 0, 1, 0)
            
#             dates = pd.to_datetime(group['date'])
#             time_indices = (dates.dt.hour * 60 + dates.dt.minute) // 5
#             time_indices = torch.tensor(time_indices.values)
#             time_embeds = self._absolute_time_encoding(time_indices).numpy()

#             num_points = len(group)
#             for i in range(0, num_points - self.seq_len + 1, self.stride):
#                 slice_gt = gt_vals[i : i + self.seq_len]
#                 if np.isnan(slice_gt).any(): continue

#                 slice_in = in_vals[i : i + self.seq_len]
#                 slice_meal = meal_vals[i : i + self.seq_len]
#                 slice_time = time_embeds[i : i + self.seq_len]
                
#                 # Normalize
#                 norm_in = self._skew_norm(slice_in) 
#                 norm_gt = self._skew_norm(slice_gt)
                
#                 sample = np.column_stack([norm_in, slice_meal, slice_time, norm_gt]) # [Input, Sin, Cos, Truth]
#                 samples.append(sample)
                
#         return np.array(samples)

    
#     def __len__(self):
#         return len(self.samples)
#     def __getitem__(self, idx):
#         return torch.FloatTensor(self.samples[idx])








# # import torch
# # import math
# # import numpy as np
# # import pandas as pd

# # def apply_mnar_values(drop_mask, day_df, max_points):
# #     """
# #     Simulates MNAR by masking ONLY contiguous Hyper/Hypo values.
# #     """
# #     cgm_values = day_df['cgm'].values
# #     is_extreme = (cgm_values > 150) | (cgm_values < 70)
    
# #     triggers = np.where(is_extreme)[0].tolist()
# #     np.random.shuffle(triggers)

# #     T = len(day_df)
# #     remaining = max_points - drop_mask.sum()
# #     if remaining <= 0: return drop_mask

# #     while remaining > 0 and len(triggers) > 0:
# #         loc_idx = triggers.pop()       
        
# #         if drop_mask[loc_idx]: continue
# #         L_max = np.random.randint(1, remaining + 1)        
# #         points_masked_in_this_loop = 0
        
# #         for i in range(L_max):
# #             curr = loc_idx + i
            
# #             if curr >= T: break
# #             if not is_extreme[curr]: break
# #             if drop_mask[curr]: break

# #             drop_mask[curr] = True
# #             points_masked_in_this_loop += 1
        
# #         remaining -= points_masked_in_this_loop

# #     return drop_mask

# # # def apply_mar_meal(drop_mask, day_df, max_points):
# # #     """ 
# # #     Meal simulation: Drops data segments starting at meal times.
# # #     """
# # #     triggers = np.where(day_df['meal'] != 0.0)[0]
# # #     np.random.shuffle(triggers)    
# # #     triggers = list(triggers) 

# # #     T = len(day_df)
# # #     remaining = max_points - drop_mask.sum()
# # #     if remaining <= 0: return drop_mask

# # #     while remaining > 0 and len(triggers) > 0:
# # #         start_idx = triggers.pop()
# # #         L = np.random.randint(1, remaining + 1)
        
# # #         if start_idx + L > T: continue
# # #         if drop_mask[start_idx : start_idx + L].any(): continue
# # #         drop_mask[start_idx : start_idx + L] = True
# # #         remaining -= L

# # #     return drop_mask




# # def apply_mar_meal(drop_mask, day_df, max_points):
# #     """ 
# #     HARD MODE MEAL SIMULATION
# #     """
# #     meal_indices = np.where(day_df['meal'] > 0)[0]
# #     if len(meal_indices) == 0: return drop_mask 

# #     largest_meal_idx = meal_indices[np.argmax(day_df.iloc[meal_indices]['meal'].values)]
# #     current_masked = drop_mask.sum()
# #     gap_points = max_points - current_masked
    
# #     if gap_points <= 0: return drop_mask

# #     start = largest_meal_idx
# #     end = min(start + gap_points, len(day_df))
    
# #     drop_mask[start:end] = True
    
# #     return drop_mask











# # def apply_mcar_random_noise(drop_mask, max_points):
# #     """
# #     MCAR with exact total missing = max_points
# #     """
# #     current = drop_mask.sum()
# #     remaining = max_points - current
# #     if remaining <= 0: return drop_mask

# #     safe_pos = np.where(~drop_mask)[0]
# #     remaining = min(remaining, len(safe_pos))

# #     if remaining > 0:
# #         mcar_pos = np.random.choice(safe_pos, size=remaining, replace=False)
# #         drop_mask[mcar_pos] = True

# #     return drop_mask


# # def process_single_day(day_df, max_pct, probs):
# #     df_day = day_df.copy()
# #     total_points = len(df_day)
# #     drop_mask = np.zeros(total_points, dtype=bool)
    
# #     max_points = int(total_points * max_pct)
    
# #     active_mnar     = np.random.rand() < probs.get('mnar', 0.5)
# #     active_meal     = np.random.rand() < probs.get('meal', 0.3)
# #     active_mcar     = np.random.rand() < probs.get('mcar', 0.8)
    
# #     if not (active_mnar or active_meal or active_mcar): return df_day

# #     # Apply Logic
# #     if active_mnar:
# #         drop_mask = apply_mnar_values(drop_mask, df_day, max_points)
# #     if active_meal:
# #         drop_mask = apply_mar_meal(drop_mask, df_day, max_points)
# #     if active_mcar:
# #         drop_mask = apply_mcar_random_noise(drop_mask, max_points)

# #     if drop_mask.sum() != max_points: return None
    
# #     col_loc = df_day.columns.get_loc('cgm_simulated')
# #     df_day.iloc[drop_mask, col_loc] = np.nan
# #     return df_day


# # def simulate_missingness_pipeline(df, max_pct, probs):
# #     df = df.copy()
# #     df['cgm_simulated'] = df['cgm'].copy()    
# #     return process_single_day(df, max_pct, probs)



# import numpy as np
# import pandas as pd

# POINTS_PER_HOUR = 12  
# MASK_WINDOW = int(1.5 * POINTS_PER_HOUR)  

# def apply_protocol_A_homeostatic(drop_mask, day_df):
#     """
#     Protocol A: The 'Steady-State' Mask (Homeostatic Regime).
#     Target: Windows defined by metabolic stability (Sleep or Fasting).
#     Condition: No meals (u_t = 0) and low variance.
#     Action: Mask 90-minute segments.
#     """
#     cgm_values = day_df['cgm'].values
#     meal_values = day_df['meal'].values
#     T = len(day_df)
    
#     valid_starts = []
    
#     gradients = np.abs(np.gradient(cgm_values))
#     is_stable = gradients < 2.0 
    
#     for t in range(T - MASK_WINDOW):
#         window_meal = meal_values[t : t + MASK_WINDOW]
#         window_stable = is_stable[t : t + MASK_WINDOW]
        
#         if (np.sum(window_meal) == 0) and (np.mean(window_stable) > 0.8):
#             valid_starts.append(t)
            
#     if not valid_starts: return drop_mask 
    
#     start_idx = np.random.choice(valid_starts)
#     drop_mask[start_idx : start_idx + MASK_WINDOW] = True
    
#     return drop_mask

# def apply_protocol_B_hidden_peak(drop_mask, day_df):
#     """
#     Protocol B: The 'Hidden Peak' Mask (Causal Regime).
#     Target: Post-prandial peaks.
#     Action: Mask 90-minute segments CENTERED on the peak following a meal.
#     """
#     cgm_values = day_df['cgm'].values
#     meal_indices = np.where(day_df['meal'] > 0)[0]
    
#     if len(meal_indices) == 0:
#         return drop_mask

#     meal_idx = np.random.choice(meal_indices)    
#     look_ahead = 24 
#     search_end = min(meal_idx + look_ahead, len(day_df))
    
#     if search_end <= meal_idx:
#         return drop_mask
        
#     local_window = cgm_values[meal_idx : search_end]
#     peak_offset = np.argmax(local_window)
#     peak_idx = meal_idx + peak_offset


#     MASK_WINDOW = 40
    
#     half_window = MASK_WINDOW // 2
#     start_idx = max(0, peak_idx - half_window)
#     end_idx = min(len(day_df), peak_idx + half_window)
    
#     if not drop_mask[start_idx:end_idx].any():
#         drop_mask[start_idx:end_idx] = True
        
#     return drop_mask

# def apply_protocol_C_hidden_rescue(drop_mask, day_df):
#     """
#     Protocol C: The 'Hidden Rescue' Mask (Hypo Detection).
#     Target: Nadir of hypoglycemic event (<70 mg/dL).
#     Action: Mask the drop/nadir to test if model infers rescue from suspension.
#     """
#     cgm_values = day_df['cgm'].values
    
#     hypo_indices = np.where(cgm_values < 70)[0]
    
#     if len(hypo_indices) == 0:
#         return drop_mask
        
#     nadir_idx = hypo_indices[np.argmin(cgm_values[hypo_indices])]
    
#     half_window = MASK_WINDOW // 2
#     start_idx = max(0, nadir_idx - half_window)
#     end_idx = min(len(day_df), nadir_idx + half_window)
    
#     drop_mask[start_idx:end_idx] = True
    
#     return drop_mask

# def process_single_day_experiment(day_df, experiment_mode='A'):
#     """
#     Master function to apply specific experimental protocols.
#     mode: 'A' (Homeostatic), 'B' (Hidden Peak), 'C' (Hidden Rescue), or 'Mixed'
#     """
#     df_day = day_df.copy()
#     total_points = len(df_day)
#     drop_mask = np.zeros(total_points, dtype=bool)
    
#     if experiment_mode == 'A':
#         drop_mask = apply_protocol_A_homeostatic(drop_mask, df_day)
#     elif experiment_mode == 'B':
#         drop_mask = apply_protocol_B_hidden_peak(drop_mask, df_day)
#     elif experiment_mode == 'C':
#         drop_mask = apply_protocol_C_hidden_rescue(drop_mask, df_day)
#     elif experiment_mode == 'Mixed':
#         if np.random.rand() < 0.33:
#             drop_mask = apply_protocol_C_hidden_rescue(drop_mask, df_day)
#         if np.random.rand() < 0.5: # Conditional on previous
#              drop_mask = apply_protocol_B_hidden_peak(drop_mask, df_day)
#         else:
#              drop_mask = apply_protocol_A_homeostatic(drop_mask, df_day)

#     # Apply NaNs
#     col_loc = df_day.columns.get_loc('cgm_simulated')
#     df_day.iloc[drop_mask, col_loc] = np.nan
    
#     return df_day

# def simulate_experiment_pipeline(df, mode='Mixed'):
#     df = df.copy()
#     if 'cgm_simulated' not in df.columns:
#         df['cgm_simulated'] = df['cgm'].copy()
#     return process_single_day_experiment(df, experiment_mode=mode)

