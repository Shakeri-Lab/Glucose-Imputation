{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e029ce6-ce45-44ad-ab2e-51148b85b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas._libs.tslibs import parsing\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def ensure_io(raw_dir: Path, output_dir: Path) -> Tuple[Path, Path]:\n",
    "    if not raw_dir.exists():\n",
    "        raise FileNotFoundError(f\"Expected raw dataset at {raw_dir}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return raw_dir, output_dir\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PEDAPConfig:\n",
    "    dedup_window_seconds: int = 15\n",
    "    sequence_gap_seconds: int = 14400  # 30 minutes\n",
    "    resample_freq_minutes: int = 5\n",
    "    interp_min_gap_seconds: int = 420  # informational\n",
    "    basal_window_before_seconds: int = 10800  # 3 hours\n",
    "    basal_window_after_seconds: int = 15\n",
    "    bolus_window_before_seconds: int = 285\n",
    "    bolus_window_after_seconds: int = 15\n",
    "    max_bolus_gap_seconds: int = 43200  # 12 hours\n",
    "    min_sequence_steps: int = 312  # 24h history + 2h horizon at 5-min steps\n",
    "\n",
    "\n",
    "def _parse_datetime(series: pd.Series) -> pd.Series:\n",
    "    sample = next((x for x in series.dropna().astype(str) if x.strip()), None)\n",
    "    if sample:\n",
    "        fmt = parsing.guess_datetime_format(sample)\n",
    "        if fmt:\n",
    "            return pd.to_datetime(series, format=fmt, errors=\"coerce\")\n",
    "    return pd.to_datetime(series, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def _read_csv_fallback(path: Path, sep: str = \"|\") -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, sep=sep, low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, sep=sep, low_memory=False, encoding=\"utf-16\")\n",
    "\n",
    "\n",
    "def _read_cgm_sources(raw_dir: Path) -> Tuple[pd.DataFrame, int]:\n",
    "    base = raw_dir / \"Data Files\"\n",
    "    frames: List[pd.DataFrame] = []\n",
    "\n",
    "    # Dexcom Clarity\n",
    "    p = base / \"PEDAPDexcomClarityCGM.txt\"\n",
    "    if p.exists():\n",
    "        df = _read_csv_fallback(p, sep=\"|\")\n",
    "        df = df.rename(columns={\"PtID\": \"pat_id\", \"DeviceDtTm\": \"date\", \"CGM\": \"cgm\"})\n",
    "        df = df[[\"pat_id\", \"date\", \"cgm\"]]\n",
    "        df[\"date\"] = _parse_datetime(df[\"date\"])\n",
    "        df[\"pat_id\"] = df[\"pat_id\"].astype(str).str.strip()\n",
    "        df[\"cgm\"] = pd.to_numeric(df[\"cgm\"], errors=\"coerce\")\n",
    "        frames.append(df)\n",
    "\n",
    "    # Tandem CGM\n",
    "    p = base / \"PEDAPTandemCGMDATAGXB.txt\"\n",
    "    if p.exists():\n",
    "        df = _read_csv_fallback(p, sep=\"|\")\n",
    "        df = df.rename(columns={\"PtID\": \"pat_id\", \"DeviceDtTm\": \"date\", \"CGMValue\": \"cgm\"})\n",
    "        df = df[[\"pat_id\", \"date\", \"cgm\"]]\n",
    "        df[\"date\"] = _parse_datetime(df[\"date\"])\n",
    "        df[\"pat_id\"] = df[\"pat_id\"].astype(str).str.strip()\n",
    "        df[\"cgm\"] = pd.to_numeric(df[\"cgm\"], errors=\"coerce\")\n",
    "        frames.append(df)\n",
    "\n",
    "    raw_total = sum(len(f) for f in frames)\n",
    "    combined = pd.concat(frames, ignore_index=True, sort=False) if frames else pd.DataFrame(columns=[\"pat_id\", \"date\", \"cgm\"])\n",
    "    combined = combined.dropna(subset=[\"pat_id\", \"date\", \"cgm\"])\n",
    "    return combined, raw_total\n",
    "\n",
    "\n",
    "def _dedup_within_window(df: pd.DataFrame, window_seconds: int) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df_sorted = df.sort_values([\"pat_id\", \"date\"])\n",
    "    grouped = df_sorted.groupby(\"pat_id\", sort=False)\n",
    "    parts: List[pd.DataFrame] = []\n",
    "    for _, g in tqdm(grouped, total=len(grouped), desc=\"Dedup PEDAP patients\"):\n",
    "        deltas = g[\"date\"].diff().dt.total_seconds()\n",
    "        new_group = (deltas.isna()) | (deltas > window_seconds)\n",
    "        group_ids = new_group.cumsum()\n",
    "        parts.append(g.groupby(group_ids).tail(1))\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "\n",
    "def _resample_and_interpolate_cgm(df: pd.DataFrame, cfg: PEDAPConfig) -> Tuple[pd.DataFrame, int]:\n",
    "    resampled_parts: List[pd.DataFrame] = []\n",
    "    max_gap = pd.Timedelta(seconds=cfg.sequence_gap_seconds)\n",
    "    freq = f\"{cfg.resample_freq_minutes}min\"\n",
    "    segment_count = 0\n",
    "    for pat_id, group in df.groupby(\"pat_id\", sort=False):\n",
    "        g = group.sort_values(\"date\")\n",
    "        gaps = g[\"date\"].diff() > max_gap\n",
    "        bounds = np.where(gaps)[0].tolist() + [len(g)]\n",
    "        start = 0\n",
    "        for end in bounds:\n",
    "            seg = g.iloc[start:end]\n",
    "            start = end\n",
    "            if seg.empty:\n",
    "                continue\n",
    "            res = seg.set_index(\"date\").sort_index()[[\"cgm\"]].resample(freq).mean()\n",
    "            res[\"pat_id\"] = pat_id\n",
    "            res[\"date\"] = res.index\n",
    "            resampled_parts.append(res.reset_index(drop=True))\n",
    "            segment_count += 1\n",
    "    if not resampled_parts:\n",
    "        return pd.DataFrame(columns=[\"pat_id\", \"date\", \"cgm\"]), 0\n",
    "    resampled = pd.concat(resampled_parts, ignore_index=True)\n",
    "    return resampled[[\"pat_id\", \"date\", \"cgm\"]], segment_count\n",
    "\n",
    "\n",
    "\n",
    "def _read_basal(raw_dir: Path) -> pd.DataFrame:\n",
    "    path = raw_dir / \"Data Files\" / \"PEDAPTandemBASALDELIVERY.txt\"\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame(columns=[\"pat_id\", \"date\", \"basal\"])\n",
    "    df = _read_csv_fallback(path, sep=\"|\")\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"PtID\": \"pat_id\",\n",
    "            \"DeviceDtTm\": \"date_raw\",\n",
    "            \"BasalRate\": \"basal\",\n",
    "        }\n",
    "    )\n",
    "    df[\"date\"] = _parse_datetime(df[\"date_raw\"])\n",
    "    df[\"pat_id\"] = df[\"pat_id\"].astype(str).str.strip()\n",
    "    df[\"basal\"] = pd.to_numeric(df[\"basal\"], errors=\"coerce\")\n",
    "    return df.dropna(subset=[\"pat_id\", \"date\", \"basal\"])\n",
    "\n",
    "\n",
    "def _attach_basal(cgm_df: pd.DataFrame, basal_df: pd.DataFrame, cfg: PEDAPConfig) -> Tuple[pd.DataFrame, int]:\n",
    "    if cgm_df.empty or basal_df.empty:\n",
    "        cgm_df = cgm_df.copy()\n",
    "        cgm_df[\"basal\"] = pd.NA\n",
    "        return cgm_df, 0\n",
    "    parts: List[pd.DataFrame] = []\n",
    "    matched = 0\n",
    "    for pat_id, group in cgm_df.groupby(\"pat_id\", sort=False):\n",
    "        g = group.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "        bg = basal_df[basal_df[\"pat_id\"] == pat_id].dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "        if g.empty:\n",
    "            continue\n",
    "        if bg.empty:\n",
    "            g = g.copy()\n",
    "            g[\"basal\"] = pd.NA\n",
    "            parts.append(g)\n",
    "            continue\n",
    "        bt = bg[\"date\"].values.astype(\"datetime64[ns]\").view(\"int64\")\n",
    "        bv = bg[\"basal\"].to_numpy()\n",
    "        ct = g[\"date\"].values.astype(\"datetime64[ns]\").view(\"int64\")\n",
    "        lower = ct - cfg.basal_window_before_seconds * 1_000_000_000\n",
    "        upper = ct + cfg.basal_window_after_seconds * 1_000_000_000\n",
    "        start = np.searchsorted(bt, lower, side=\"left\")\n",
    "        end = np.searchsorted(bt, upper, side=\"right\")\n",
    "        chosen = np.where(start < end, end - 1, -1)\n",
    "        basal_vals = np.where(chosen >= 0, bv[chosen], np.nan)\n",
    "        matched += np.isfinite(basal_vals).sum()\n",
    "        g = g.copy()\n",
    "        g[\"basal\"] = basal_vals\n",
    "        parts.append(g)\n",
    "    if not parts:\n",
    "        cgm_df = cgm_df.copy()\n",
    "        cgm_df[\"basal\"] = pd.NA\n",
    "        return cgm_df, 0\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "    return out, int(matched)\n",
    "\n",
    "\n",
    "def _read_bolus(raw_dir: Path) -> pd.DataFrame:\n",
    "    path = raw_dir / \"Data Files\" / \"PEDAPTandemBolusDelivered.txt\"\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame(columns=[\"pat_id\", \"date\", \"bolus\", \"bolus_type\"])\n",
    "    df = _read_csv_fallback(path, sep=\"|\")\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"PtID\": \"pat_id\",\n",
    "            \"DeviceDtTm\": \"date_raw\",\n",
    "            \"BolusAmount\": \"bolus\",\n",
    "            \"BolusType\": \"bolus_type\",\n",
    "        }\n",
    "    )\n",
    "    df[\"date\"] = _parse_datetime(df[\"date_raw\"])\n",
    "    df[\"pat_id\"] = df[\"pat_id\"].astype(str).str.strip()\n",
    "    df[\"bolus\"] = pd.to_numeric(df[\"bolus\"], errors=\"coerce\")\n",
    "    df[\"bolus_type\"] = df[\"bolus_type\"].fillna(\"Unknown\")\n",
    "    return df.dropna(subset=[\"pat_id\", \"date\", \"bolus\"])\n",
    "\n",
    "\n",
    "def _attach_bolus(cgm_df: pd.DataFrame, bolus_df: pd.DataFrame, cfg: PEDAPConfig) -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
    "    cgm_df = cgm_df.copy()\n",
    "    cgm_df[\"bolus\"] = 0.0\n",
    "    cgm_df[\"bolus_standard\"] = 0.0\n",
    "    cgm_df[\"bolus_extended\"] = 0.0\n",
    "    counts: Dict[str, int] = {}\n",
    "    if cgm_df.empty or bolus_df.empty:\n",
    "        return cgm_df, {\"total\": 0}\n",
    "    for pat_id, group in tqdm(cgm_df.groupby(\"pat_id\", sort=False), total=cgm_df[\"pat_id\"].nunique(), desc=\"Attach bolus to CGM\"):\n",
    "        bg = bolus_df[bolus_df[\"pat_id\"] == pat_id].sort_values(\"date\")\n",
    "        if bg.empty:\n",
    "            continue\n",
    "        bt = bg[\"date\"].values.astype(\"datetime64[ns]\").view(\"int64\")\n",
    "        bv = bg[\"bolus\"].to_numpy()\n",
    "        btype = bg[\"bolus_type\"].to_numpy()\n",
    "        ct = group[\"date\"].values.astype(\"datetime64[ns]\").view(\"int64\")\n",
    "        idx_right = np.searchsorted(bt, ct, side=\"left\")\n",
    "        prev_idx = idx_right - 1\n",
    "        next_idx = idx_right\n",
    "        valid_prev = prev_idx >= 0\n",
    "        valid_next = next_idx < len(bt)\n",
    "        prev_dist = np.where(valid_prev, ct - bt[np.clip(prev_idx, 0, len(bt)-1)], np.inf)\n",
    "        next_dist = np.where(valid_next, bt[np.clip(next_idx, 0, len(bt)-1)] - ct, np.inf)\n",
    "        prev_dist = np.where(prev_dist <= cfg.bolus_window_before_seconds * 1_000_000_000, prev_dist, np.inf)\n",
    "        next_dist = np.where(next_dist <= cfg.bolus_window_after_seconds * 1_000_000_000, next_dist, np.inf)\n",
    "        choose_prev = prev_dist < next_dist\n",
    "        best_idx = np.where(np.isfinite(prev_dist) | np.isfinite(next_dist),\n",
    "                            np.where(choose_prev, prev_idx, next_idx),\n",
    "                            -1)\n",
    "        amounts = []\n",
    "        bol_std = []\n",
    "        bol_ext = []\n",
    "        for bi in best_idx:\n",
    "            if bi == -1:\n",
    "                amounts.append(0.0)\n",
    "                bol_std.append(0.0)\n",
    "                bol_ext.append(0.0)\n",
    "            else:\n",
    "                amt = float(bv[bi])\n",
    "                amounts.append(amt)\n",
    "                t = btype[bi]\n",
    "                is_extended = isinstance(t, str) and (\"extend\" in t.lower() or \"square\" in t.lower() or \"dual\" in t.lower())\n",
    "                if is_extended:\n",
    "                    bol_ext.append(amt)\n",
    "                    bol_std.append(0.0)\n",
    "                else:\n",
    "                    bol_std.append(amt)\n",
    "                    bol_ext.append(0.0)\n",
    "                if t:\n",
    "                    counts[t] = counts.get(t, 0) + (1 if amt != 0 else 0)\n",
    "        cgm_df.loc[group.index, \"bolus\"] = amounts\n",
    "        cgm_df.loc[group.index, \"bolus_standard\"] = bol_std\n",
    "        cgm_df.loc[group.index, \"bolus_extended\"] = bol_ext\n",
    "    counts[\"total\"] = sum(counts.values())\n",
    "    return cgm_df, counts\n",
    "\n",
    "\n",
    "def _filter_long_bolus_gaps(df: pd.DataFrame, cfg: PEDAPConfig) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    out = []\n",
    "    th = pd.Timedelta(seconds=cfg.max_bolus_gap_seconds)\n",
    "    for pat_id, g in df.groupby(\"pat_id\", sort=False):\n",
    "        g = g.sort_values(\"date\")\n",
    "        last = None\n",
    "        keep = []\n",
    "        for ts, bol_std, bol_ext in zip(g[\"date\"], g.get(\"bolus_standard\", g[\"bolus\"]), g.get(\"bolus_extended\", 0)):\n",
    "            bol_total = (bol_std or 0) + (bol_ext or 0)\n",
    "            if bol_total and bol_total != 0:\n",
    "                last = ts\n",
    "                keep.append(True)\n",
    "            else:\n",
    "                if last is None:\n",
    "                    keep.append(False)\n",
    "                else:\n",
    "                    keep.append((ts - last) <= th)\n",
    "        kept = g.loc[keep]\n",
    "        if not kept.empty:\n",
    "            out.append(kept)\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame(columns=df.columns)\n",
    "\n",
    "\n",
    "def _read_meal(raw_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Use Tandem bolus delivered carb entries as meal events.\"\"\"\n",
    "    path = raw_dir / \"Data Files\" / \"PEDAPTandemBolusDelivered.txt\"\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame(columns=[\"pat_id\", \"date\", \"meal\"])\n",
    "    df = pd.read_csv(path, sep=\"|\", low_memory=False)\n",
    "    df = df.rename(columns={\"PtID\": \"pat_id\", \"DeviceDtTm\": \"date\", \"CarbAmount\": \"meal\"})\n",
    "    df[\"date\"] = _parse_datetime(df[\"date\"])\n",
    "    df[\"pat_id\"] = df[\"pat_id\"].astype(str).str.strip()\n",
    "    df[\"meal\"] = pd.to_numeric(df[\"meal\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"pat_id\", \"date\", \"meal\"])\n",
    "    df = df[df[\"meal\"] > 0]\n",
    "    return df[[\"pat_id\", \"date\", \"meal\"]]\n",
    "\n",
    "\n",
    "def _attach_meal(df: pd.DataFrame, meal_df: pd.DataFrame, cfg: PEDAPConfig) -> pd.DataFrame:\n",
    "    if df.empty or meal_df.empty:\n",
    "        df = df.copy()\n",
    "        df[\"meal\"] = 0.0\n",
    "        return df\n",
    "    freq = f\"{cfg.resample_freq_minutes}min\"\n",
    "    meal_df = meal_df.copy()\n",
    "    meal_df[\"date_round\"] = meal_df[\"date\"].dt.round(freq)\n",
    "    meal_agg = (\n",
    "        meal_df.groupby([\"pat_id\", \"date_round\"], as_index=False)[\"meal\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"date_round\": \"date\", \"meal\": \"meal_val\"})\n",
    "    )\n",
    "    out = df.merge(meal_agg, on=[\"pat_id\", \"date\"], how=\"left\")\n",
    "    out[\"meal\"] = out[\"meal_val\"].fillna(0.0)\n",
    "    out = out.drop(columns=[\"meal_val\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "def _assign_sequences(df: pd.DataFrame, cfg: PEDAPConfig) -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
    "    if df.empty:\n",
    "        cols = list(df.columns)\n",
    "        if \"seq_id\" not in cols:\n",
    "            cols.append(\"seq_id\")\n",
    "        return pd.DataFrame(columns=cols), {\"sequences_total\": 0, \"sequences_kept\": 0, \"sequences_dropped_short\": 0}\n",
    "    max_gap = pd.Timedelta(seconds=cfg.sequence_gap_seconds)\n",
    "    seq_id = 1\n",
    "    kept: List[pd.DataFrame] = []\n",
    "    total = dropped = 0\n",
    "    for pat_id, g in df.groupby(\"pat_id\", sort=False):\n",
    "        g = g.sort_values(\"date\")\n",
    "        gaps = g[\"date\"].diff() > max_gap\n",
    "        bounds = np.where(gaps)[0].tolist() + [len(g)]\n",
    "        s = 0\n",
    "        for e in bounds:\n",
    "            seg = g.iloc[s:e]\n",
    "            s = e\n",
    "            if seg.empty:\n",
    "                continue\n",
    "            total += 1\n",
    "            if len(seg) < cfg.min_sequence_steps:\n",
    "                dropped += 1\n",
    "                continue\n",
    "            seg = seg.copy()\n",
    "            seg[\"seq_id\"] = seq_id\n",
    "            seq_id += 1\n",
    "            kept.append(seg)\n",
    "    stats = {\"sequences_total\": total, \"sequences_kept\": len(kept), \"sequences_dropped_short\": dropped}\n",
    "    return (pd.concat(kept, ignore_index=True) if kept else pd.DataFrame(columns=df.columns)), stats\n",
    "\n",
    "\n",
    "def _read_weight_sources(raw_dir: Path, cgm_min_dates: Dict[str, pd.Timestamp]) -> pd.DataFrame:\n",
    "    records: List[Dict[str, object]] = []\n",
    "    base = raw_dir / \"Data Files\"\n",
    "\n",
    "    def add_records(path: Path, date_col: str | None, weight_col: str, unit_col: str):\n",
    "        if not path.exists():\n",
    "            return\n",
    "        df = _read_csv_fallback(path, sep=\"|\")\n",
    "        if \"PtID\" not in df.columns or weight_col not in df.columns:\n",
    "            return\n",
    "        df = df.rename(columns={\"PtID\": \"pat_id\", weight_col: \"weight\", unit_col: \"units\"})\n",
    "        df[\"pat_id\"] = df[\"pat_id\"].astype(str).str.strip()\n",
    "        df[\"weight\"] = pd.to_numeric(df[\"weight\"], errors=\"coerce\")\n",
    "        df[\"units\"] = df[\"units\"].astype(str).str.lower()\n",
    "        if date_col and date_col in df.columns:\n",
    "            df[\"date\"] = _parse_datetime(df[date_col])\n",
    "        else:\n",
    "            df[\"date\"] = pd.NaT\n",
    "        for _, row in df.dropna(subset=[\"weight\"]).iterrows():\n",
    "            records.append(row.to_dict())\n",
    "\n",
    "    add_records(base / \"PEDAPFollowUpCTV.txt\", \"CGMUploadedDt\", \"Weight\", \"WeightUnits\")\n",
    "    if (base / \"PEDAPDiabScreening.txt\").exists():\n",
    "        scr = _read_csv_fallback(base / \"PEDAPDiabScreening.txt\", sep=\"|\")\n",
    "        if \"PtID\" in scr.columns and \"Weight\" in scr.columns:\n",
    "            scr = scr.rename(columns={\"PtID\": \"pat_id\", \"Weight\": \"weight\", \"WeightUnits\": \"units\"})\n",
    "            scr[\"pat_id\"] = scr[\"pat_id\"].astype(str).str.strip()\n",
    "            scr[\"weight\"] = pd.to_numeric(scr[\"weight\"], errors=\"coerce\")\n",
    "            scr[\"units\"] = scr[\"units\"].astype(str).str.lower()\n",
    "            for _, row in scr.dropna(subset=[\"weight\"]).iterrows():\n",
    "                pat = row[\"pat_id\"]\n",
    "                anchor = cgm_min_dates.get(pat)\n",
    "                if anchor is None:\n",
    "                    continue\n",
    "                records.append({\"pat_id\": pat, \"date\": anchor, \"weight\": row[\"weight\"], \"units\": row[\"units\"]})\n",
    "\n",
    "    phys_path = base / \"PEDAPDiabPhysExam.txt\"\n",
    "    if phys_path.exists():\n",
    "        phys = _read_csv_fallback(phys_path, sep=\"|\")\n",
    "        if {\"PtID\", \"Weight\"}.issubset(phys.columns):\n",
    "            phys = phys.rename(columns={\"PtID\": \"pat_id\", \"Weight\": \"weight\", \"WeightUnits\": \"units\"})\n",
    "            phys[\"pat_id\"] = phys[\"pat_id\"].astype(str).str.strip()\n",
    "            phys[\"weight\"] = pd.to_numeric(phys[\"weight\"], errors=\"coerce\")\n",
    "            phys[\"units\"] = phys.get(\"units\", phys.get(\"WeightUnits\", pd.Series(index=phys.index))).astype(str).str.lower()\n",
    "            for pat, grp in phys.dropna(subset=[\"weight\"]).groupby(\"pat_id\"):\n",
    "                anchor = cgm_min_dates.get(pat)\n",
    "                if anchor is None:\n",
    "                    continue\n",
    "                grp_sorted = grp.sort_values(\"RecID\") if \"RecID\" in grp.columns else grp\n",
    "                for offset, (_, row) in enumerate(grp_sorted.iterrows()):\n",
    "                    records.append(\n",
    "                        {\n",
    "                            \"pat_id\": pat,\n",
    "                            \"date\": anchor + pd.Timedelta(days=offset),\n",
    "                            \"weight\": row[\"weight\"],\n",
    "                            \"units\": row[\"units\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    adv_path = base / \"PEDAPAdvEvent.txt\"\n",
    "    if adv_path.exists():\n",
    "        adv = _read_csv_fallback(adv_path, sep=\"|\")\n",
    "        date_col = None\n",
    "        for cand in [\"AENotifiedDt\", \"AEOnsetDt\", \"AEResDt\"]:\n",
    "            if cand in adv.columns:\n",
    "                date_col = cand\n",
    "                break\n",
    "        if {\"PtID\", \"Weight\"}.issubset(adv.columns) and date_col:\n",
    "            adv = adv.rename(columns={\"PtID\": \"pat_id\", \"Weight\": \"weight\"})\n",
    "            adv[\"pat_id\"] = adv[\"pat_id\"].astype(str).str.strip()\n",
    "            adv[\"weight\"] = pd.to_numeric(adv[\"weight\"], errors=\"coerce\")\n",
    "            adv[\"date\"] = _parse_datetime(adv[date_col])\n",
    "            adv = adv.dropna(subset=[\"weight\", \"date\"])\n",
    "            for _, row in adv.iterrows():\n",
    "                records.append({\"pat_id\": row[\"pat_id\"], \"date\": row[\"date\"], \"weight\": row[\"weight\"], \"units\": \"kg\"})\n",
    "\n",
    "    if not records:\n",
    "        return pd.DataFrame(columns=[\"pat_id\", \"date\", \"weight_kg\"])\n",
    "\n",
    "    weight_df = pd.DataFrame(records)\n",
    "\n",
    "    def to_kg(w, units):\n",
    "        if pd.isna(w):\n",
    "            return np.nan\n",
    "        if isinstance(units, str) and \"lb\" in units:\n",
    "            return float(w) * 0.45359237\n",
    "        return float(w)\n",
    "\n",
    "    weight_df[\"weight_kg\"] = weight_df.apply(lambda r: to_kg(r[\"weight\"], r[\"units\"]), axis=1)\n",
    "    weight_df = weight_df.dropna(subset=[\"pat_id\", \"date\", \"weight_kg\"])\n",
    "    return weight_df[[\"pat_id\", \"date\", \"weight_kg\"]]\n",
    "\n",
    "\n",
    "def _attach_weight(cgm_df: pd.DataFrame, weight_df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
    "    cgm_df = cgm_df.copy()\n",
    "    cgm_df[\"weight_kg\"] = pd.NA\n",
    "    stats = {\"rows_with_weight\": 0, \"rows_weight_interpolated\": 0}\n",
    "    if cgm_df.empty or weight_df.empty:\n",
    "        return cgm_df, stats\n",
    "    for pat_id, g in cgm_df.groupby(\"pat_id\", sort=False):\n",
    "        w = weight_df[weight_df[\"pat_id\"] == pat_id].sort_values(\"date\")\n",
    "        if w.empty:\n",
    "            continue\n",
    "        times = w[\"date\"].values.astype(\"datetime64[ns]\").view(\"int64\")\n",
    "        weights = w[\"weight_kg\"].to_numpy(dtype=float)\n",
    "        def find(ts: pd.Timestamp):\n",
    "            ts_ns = ts.value\n",
    "            idx = times.searchsorted(ts_ns)\n",
    "            if idx == 0:\n",
    "                return weights[0], False\n",
    "            if idx == len(times):\n",
    "                return weights[-1], False\n",
    "            t0, t1 = times[idx - 1], times[idx]\n",
    "            w0, w1 = weights[idx - 1], weights[idx]\n",
    "            if t1 == t0:\n",
    "                return w1, False\n",
    "            frac = (ts_ns - t0) / (t1 - t0)\n",
    "            return w0 + frac * (w1 - w0), True\n",
    "        vals = []\n",
    "        interp_flags = []\n",
    "        for ts in g[\"date\"]:\n",
    "            wv, inter = find(ts)\n",
    "            vals.append(wv)\n",
    "            interp_flags.append(inter)\n",
    "        stats[\"rows_with_weight\"] += len(g)\n",
    "        stats[\"rows_weight_interpolated\"] += sum(interp_flags)\n",
    "        cgm_df.loc[g.index, \"weight_kg\"] = vals\n",
    "    return cgm_df, stats\n",
    "\n",
    "\n",
    "def preprocess_pedap(raw_dir: Path, output_dir: Path) -> None:\n",
    "    raw_dir, output_dir = ensure_io(raw_dir, output_dir)\n",
    "    cfg = PEDAPConfig()\n",
    "\n",
    "    combined, raw_total = _read_cgm_sources(raw_dir)\n",
    "    valid_count = len(combined)\n",
    "    deduped = _dedup_within_window(combined, cfg.dedup_window_seconds)\n",
    "    resampled, segment_count = _resample_and_interpolate_cgm(deduped, cfg)\n",
    "\n",
    "    basal_df = _read_basal(raw_dir)\n",
    "    with_basal, basal_matched = _attach_basal(resampled, basal_df, cfg)\n",
    "\n",
    "    bolus_df = _read_bolus(raw_dir)\n",
    "    with_bolus, bolus_counts = _attach_bolus(with_basal, bolus_df, cfg)\n",
    "\n",
    "    bolus_filtered = _filter_long_bolus_gaps(with_bolus, cfg)\n",
    "    cgm_min_dates = resampled.groupby(\"pat_id\")[\"date\"].min().to_dict() if not resampled.empty else {}\n",
    "    weight_df = _read_weight_sources(raw_dir, cgm_min_dates)\n",
    "    with_weight, weight_stats = _attach_weight(bolus_filtered, weight_df)\n",
    "\n",
    "    meal_df = _read_meal(raw_dir)\n",
    "    with_meal = _attach_meal(with_weight, meal_df, cfg)\n",
    "\n",
    "    final_df, seq_stats = _assign_sequences(with_meal, cfg)\n",
    "\n",
    "    metadata = {\n",
    "        \"dataset\": \"PEDAP\",\n",
    "        \"source_files\": [\n",
    "            \"Data Files/PEDAPDexcomClarityCGM.txt\",\n",
    "            \"Data Files/PEDAPTandemCGMDATAGXB.txt\",\n",
    "            \"Data Files/PEDAPTandemBASALDELIVERY.txt\",\n",
    "            \"Data Files/PEDAPTandemBolusDelivered.txt\",\n",
    "            \"Data Files/PEDAPDiabPhysExam.txt\",\n",
    "            \"Data Files/PEDAPDiabScreening.txt\",\n",
    "            \"Data Files/PEDAPFollowUpCTV.txt\",\n",
    "            \"Data Files/PEDAPAdvEvent.txt\",\n",
    "        ],\n",
    "        \"raw_rows\": int(raw_total),\n",
    "        \"rows_after_validation\": int(valid_count),\n",
    "        \"rows_after_dedup\": int(len(deduped)),\n",
    "        \"rows_after_resample\": int(len(resampled)),\n",
    "        \"rows_after_bolus_gap_filter\": int(len(bolus_filtered)),\n",
    "        \"rows_final\": int(len(final_df)),\n",
    "        \"rows_with_basal\": int(basal_matched),\n",
    "        \"rows_with_bolus\": int((final_df[\"bolus_standard\"] != 0).sum() + (final_df[\"bolus_extended\"] != 0).sum()),\n",
    "        \"patients_final\": int(final_df[\"pat_id\"].nunique()) if not final_df.empty else 0,\n",
    "        \"patients_with_weight\": int(final_df.dropna(subset=[\"weight_kg\"])[\"pat_id\"].nunique()) if not final_df.empty else 0,\n",
    "        \"bolus_counts_by_type\": bolus_counts,\n",
    "        \"weight_records\": int(len(weight_df)),\n",
    "        \"rows_with_weight\": int(weight_stats[\"rows_with_weight\"]),\n",
    "        \"rows_weight_interpolated\": int(weight_stats[\"rows_weight_interpolated\"]),\n",
    "        \"segment_count_before_resample\": int(segment_count),\n",
    "        \"sequence_stats\": seq_stats,\n",
    "        \"channels\": [\"cgm\", \"basal\", \"bolus_standard\", \"bolus_extended\", \"weight_kg\", \"meal\"],\n",
    "        \"basal_window_seconds_before\": cfg.basal_window_before_seconds,\n",
    "        \"basal_window_seconds_after\": cfg.basal_window_after_seconds,\n",
    "        \"bolus_window_seconds_before\": cfg.bolus_window_before_seconds,\n",
    "        \"bolus_window_seconds_after\": cfg.bolus_window_after_seconds,\n",
    "        \"dedup_window_seconds\": cfg.dedup_window_seconds,\n",
    "        \"sequence_gap_seconds\": cfg.sequence_gap_seconds,\n",
    "        \"resample_freq_minutes\": cfg.resample_freq_minutes,\n",
    "        \"max_bolus_gap_seconds\": cfg.max_bolus_gap_seconds,\n",
    "        \"min_sequence_steps\": cfg.min_sequence_steps,\n",
    "        \"notes\": (\n",
    "            \"Combined CGM sources, deduped within window, resampled/interpolated, \"\n",
    "            \"attached basal/bolus, filtered long bolus gaps, attached weight, and filtered short sequences.\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    final_df = final_df.sort_values([\"pat_id\", \"date\"]).reset_index(drop=True)\n",
    "    final_df[\"date\"] = final_df[\"date\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    final_df[\"basal\"] = final_df.groupby(\"pat_id\")[\"basal\"].ffill().fillna(0.0)\n",
    "    final_df[\"meal\"] = final_df[\"meal\"].fillna(0.0)\n",
    "    if \"bolus_standard\" not in final_df.columns:\n",
    "        final_df[\"bolus_standard\"] = final_df.get(\"bolus\", 0)\n",
    "    if \"bolus_extended\" not in final_df.columns:\n",
    "        final_df[\"bolus_extended\"] = 0.0\n",
    "    metadata[\"meals_found\"] = int((final_df[\"meal\"] > 0).sum())\n",
    "    final_df = final_df[\n",
    "        [\"pat_id\", \"seq_id\", \"date\", \"cgm\", \"basal\", \"bolus_standard\", \"bolus_extended\", \"weight_kg\", \"meal\"]\n",
    "    ]\n",
    "    final_df.to_csv(output_dir / \"timeseries.csv\", index=False)\n",
    "    (output_dir / \"metadata.json\").write_text(json.dumps(metadata, indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    RAW_DIR = Path(\"/project/shakeri-lab/Alireza_timeseries/benchmark/datasets_raw/PEDAP/\")\n",
    "    OUTPUT_DIR = Path(\"./PEDAP\")\n",
    "    preprocess_pedap(RAW_DIR, OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
